---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: add-fbc-contribution
  annotations:
    tekton.dev/pipelines.minVersion: "0.12.1"
    tekton.dev/tags: release
spec:
  description: |-
    Task to create internalrequests to add fbc contributions to index images. It can batch
    multiple fragments into a single IIB request and can also split requests according
    to their OCP versions.

    This task batches FBC fragments to submit them to IIB in sets of params.maxBatchSize.
    The Snapshot has been previously augmented by prepare-fbc-snapshot to include OCP and
    target index metadata for each component.

    In this task, we split fragments up by their OCP versions and then process each of these
    in series. For each OCP version, we chain together batches so that the final targetIndex
    produced will have all fragments added. This means that the index_image from one internal
    request will be set as the fromIndex for the next request within that OCP version.

    Since we have seen flakiness in IIB requests in the past, we retry and failed batches
    and internal requests can attach onto currently in progress IIB requests. We retry batches
    at the end to allow for timed out requests to finish so that we can just get the final
    result. This will slightly compress the time in which batches are entered into the IIB
    queue to reduce the effect of a full queue on a single release.
  params:
    - name: snapshotPath
      description: Path to the JSON string of the mapped Snapshot spec in the data workspace
      type: string
    - name: dataPath
      description: Path to the JSON string of the merged data to use in the data workspace
      type: string
    - name: pipelineRunUid
      type: string
      description: The uid of the current pipelineRun. Used as a label value when creating internal requests
    - name: resultsDirPath
      type: string
      description: Path to the results directory in the data workspace
    - name: ociStorage
      description: The OCI repository where the Trusted Artifacts are stored
      type: string
      default: "empty"
    - name: ociArtifactExpiresAfter
      description: Expiration date for the trusted artifacts created in the
        OCI repository. An empty string means the artifacts do not expire
      type: string
      default: "1d"
    - name: trustedArtifactsDebug
      description: Flag to enable debug logging in trusted artifacts. Set to a non-empty string to enable
      type: string
      default: ""
    - name: orasOptions
      description: oras options to pass to Trusted Artifacts calls
      type: string
      default: ""
    - name: sourceDataArtifact
      type: string
      description: Location of trusted artifacts to be used to populate data directory
      default: ""
    - name: dataDir
      description: The location where data will be stored
      type: string
      default: /var/workdir/release
    - name: taskGitUrl
      type: string
      description: The url to the git repo where the release-service-catalog tasks and stepactions to be used are stored
    - name: taskGitRevision
      type: string
      description: The revision in the taskGitUrl repo to be used
    - name: maxBatchSize
      type: string
      description: Maximum number of FBC fragments to process in a single batch
      default: "5"
    - name: mustPublishIndexImage
      type: string
      description: Whether the index image should be published (from prepare-fbc-parameters)
    - name: mustOverwriteFromIndexImage
      type: string
      description: Whether to overwrite the from index image (from prepare-fbc-parameters)
    - name: iibServiceAccountSecret
      type: string
      description: IIB service account secret name (from prepare-fbc-parameters)
    - name: maxRetries
      description: Maximum number of retry attempts for failed internal requests
      type: string
      default: "3"
    - name: batchRetryDelaySeconds
      description: Delay between batch retry attempts in seconds
      type: string
      default: "60"
  results:
    - name: buildTimestamp
      description: Build timestamp used in the tag
    - name: requestResultsFile
      description: Internal Request results file
    - name: internalRequestResultsFile
      description: Additional Internal Request results file
    - name: requestMessage
      description: Internal Request message
    - name: requestReason
      description: Internal Request reason
    - name: indexImageDigests
      description: list of manifest digests for each arch from manifest list in index image
    - name: sourceDataArtifact
      type: string
      description: Produced trusted data artifact
  volumes:
    - name: workdir
      emptyDir: {}
  stepTemplate:
    volumeMounts:
      - mountPath: /var/workdir
        name: workdir
    env:
      - name: IMAGE_EXPIRES_AFTER
        value: $(params.ociArtifactExpiresAfter)
      - name: "ORAS_OPTIONS"
        value: "$(params.orasOptions)"
      - name: "DEBUG"
        value: "$(params.trustedArtifactsDebug)"
  steps:
    - name: use-trusted-artifact
      computeResources:
        limits:
          memory: 64Mi
        requests:
          memory: 64Mi
          cpu: 30m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/use-trusted-artifact/use-trusted-artifact.yaml
      params:
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(params.sourceDataArtifact)
        - name: orasOptions
          value: $(params.orasOptions)
    - name: add-contribution
      image: quay.io/konflux-ci/release-service-utils:cee1fd0191e750b0736d8197701d91ce2f308e03
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi  # was exiting with code 137 when set to 256Mi
          cpu: 200m
      script: |
        #!/usr/bin/env bash
        #
        set -eo pipefail

        SNAPSHOT_PATH="$(params.dataDir)/$(params.snapshotPath)"
        DATA_FILE="$(params.dataDir)/$(params.dataPath)"
        if [ ! -f "${DATA_FILE}" ] ; then
            echo "No valid data file was provided."
            exit 1
        fi

        # The snapshot should be updated by prepare-fbc-snapshot
        SNAPSHOT_PATH="$(params.dataDir)/$(params.snapshotPath)"
        if [ ! -f "$SNAPSHOT_PATH" ]; then
            echo "ERROR: Snapshot not found at $SNAPSHOT_PATH"
            exit 1
        fi

        # adding a new result as modifying the current one used breaks e2e for single component.
        # to be handled in RELEASE-1640.
        RESULTS_FILE="$(params.resultsDirPath)/internal-requests-results.json"
        echo -n "$RESULTS_FILE" > "$(results.internalRequestResultsFile.path)"
        RESULTS_FILE="$(params.dataDir)/$(params.resultsDirPath)/internal-requests-results.json"

        echo -n "$(params.dataDir)/$(params.pipelineRunUid)/ir-$(context.taskRun.uid)-result.json" \
          > "$(results.requestResultsFile.path)"

        default_build_timeout_seconds="3600"
        default_request_timeout_seconds="3600"

        build_timeout_seconds=$(jq -r --arg build_timeout_seconds "${default_build_timeout_seconds}" \
            '.fbc.buildTimeoutSeconds // $build_timeout_seconds' "${DATA_FILE}")
        request_timeout_seconds=$(jq -r --arg request_timeout_seconds "${default_request_timeout_seconds}" \
            '.fbc.requestTimeoutSeconds // $request_timeout_seconds' "${DATA_FILE}")
        internal_request_service_account=$(jq -r '.fbc.internalRequestServiceAccount // "release-service-account"' \
            "${DATA_FILE}")
        publishing_credentials=$(jq -r '.fbc.publishingCredentials // "catalog-publishing-secret"' "$DATA_FILE")

        # Use pre-determined values from prepare-fbc-parameters
        iib_service_account_secret="$(params.iibServiceAccountSecret)"
        mustPublishIndexImage="$(params.mustPublishIndexImage)"
        mustOverwriteFromIndexImage="$(params.mustOverwriteFromIndexImage)"

        timestamp_format=$(jq -r '.fbc.timestampFormat // "%s"' "${DATA_FILE}")
        timestamp=$(date "+${timestamp_format}")

        # Extract OCP versions from snapshot
        ocp_versions=$(jq -r '.components[].ocpVersion' "$SNAPSHOT_PATH" | sort -u)
        echo "INFO: Found OCP versions: $(echo "$ocp_versions" | tr '\n' ' ')"

        echo "INFO: Using pre-determined values from prepare-fbc-parameters:"
        echo "  - mustPublishIndexImage: ${mustPublishIndexImage}"
        echo "  - mustOverwriteFromIndexImage: ${mustOverwriteFromIndexImage}"
        echo "  - iibServiceAccountSecret: ${iib_service_account_secret}"

        # Initialize results file for multi-OCP processing
        echo -n "$timestamp" > "$(results.buildTimestamp.path)"
        jq -n '{"components": []}' | tee "$RESULTS_FILE"

        # Snapshot validation for multi-OCP processing
        if ! jq -e '.components | type == "array" and length > 0' "$SNAPSHOT_PATH" > /dev/null; then
            echo "ERROR: Snapshot missing required 'components' array"
            exit 1
        fi

        # Validate snapshot for processing
        echo "INFO: Validating snapshot for processing..."
        total_components=$(jq '.components | length' "$SNAPSHOT_PATH")

        if [ "${total_components}" -eq 0 ]; then
            echo "ERROR: No components found in snapshot"
            exit 1
        fi

        echo "INFO: Processing ${total_components} components"

        # Setup consistent labeling for FBC internal requests
        TASK_LABEL="internal-services.appstudio.openshift.io/group-id"
        TASK_ID=$(context.taskRun.uid)
        PIPELINERUN_LABEL="internal-services.appstudio.openshift.io/pipelinerun-uid"

        # Create batches with size limits
        LENGTH=$(jq -r '.components | length' "${SNAPSHOT_PATH}")
        MAX_BATCH_SIZE="$(params.maxBatchSize)"
        echo "Processing $LENGTH components with maximum batch size of $MAX_BATCH_SIZE..."

        # Read global buildTags and addArches from data file
        build_tags=$(jq '.fbc.buildTags // []' "${DATA_FILE}")
        add_arches=$(jq '.fbc.addArches // []' "${DATA_FILE}")

        # Processing will use group-specific target indexes
        # Each OCP version group will have its own target index resolved by prepare-fbc-parameters
        # No single "global" target index exists - each component gets its OCP-specific target index

        # Calculate number of batches needed
        NUM_BATCHES=$(( (LENGTH + MAX_BATCH_SIZE - 1) / MAX_BATCH_SIZE ))
        echo "Creating $NUM_BATCHES batch(es) for $LENGTH components"

        echo "INFO: Processing snapshot components"
        # Create groups directory
        groups_dir="$(params.dataDir)/ocp-groups"
        mkdir -p "$groups_dir"

        # Calculate timeout for batches
        finally_task_timeout=300
        pipeline_timeout=$(date -u "+%Hh%Mm%Ss" -d @$(( request_timeout_seconds + finally_task_timeout )))
        task_timeout=$(date -u "+%Hh%Mm%Ss" -d @$(( request_timeout_seconds )))

        # Group components by OCP version for isolated processing
        group_components_by_ocp_version() {
            local snapshot_file="$1"
            local ocp_versions="$2"

            # Group components by OCP version
            for ocp_version in $ocp_versions; do
                local group_file="${groups_dir}/${ocp_version}.json"
                jq --arg ocp_version "$ocp_version" \
                    '.components | map(select(.ocpVersion == $ocp_version))' \
                    "$snapshot_file" > "$group_file"

                local group_count
                group_count=$(jq 'length' "$group_file")
                echo "INFO: OCP version $ocp_version has $group_count components" >&2
            done
        }

        # Group components by OCP versions
        group_components_by_ocp_version "$SNAPSHOT_PATH" "$ocp_versions"

        # Process all OCP groups sequentially
        process_all_ocp_groups() {
            echo "INFO: Starting group processing"

            for ocp_version in $ocp_versions; do
                local group_file="${groups_dir}/${ocp_version}.json"
                echo "INFO: Processing OCP group: $ocp_version"

                local group_components
                group_components=$(cat "$group_file")

                process_ocp_group "$ocp_version" "$group_components"
            done

            echo "INFO: Processing completed successfully"
        }

        # Process OCP group
        process_ocp_group() {
            local group_ocp_version="$1"
            local group_components="$2"

            echo "INFO: Processing OCP group $group_ocp_version"

            # Get group-specific parameters from first component (all should have same values within group)
            local first_component
            first_component=$(jq -r '.[0]' <<< "$group_components")
            local group_from_index
            group_from_index=$(jq -r '.updatedFromIndex // empty' <<< "$first_component")
            local group_target_index
            group_target_index=$(jq -r '.targetIndex // empty' <<< "$first_component")

            # Group parameters are already validated globally, but verify tag format
            echo "INFO: Group parameters - fromIndex: $group_from_index, targetIndex: $group_target_index"

            # Extract and validate tag from group target index for PLR identification
            # For staged releases, targetIndex is empty so no tag extraction is needed
            if [ -n "$group_target_index" ]; then
                group_target_tag=$(printf '%s' "${group_target_index}" | rev | cut -d: -f1 | rev)
                if [[ "$group_target_tag" = "${group_target_index}" ]]; then
                    echo "ERROR: Target index for OCP group $group_ocp_version has no tag: $group_target_index"
                    exit 1
                fi
                echo "INFO: Using tag '$group_target_tag' for PLR identification in OCP group $group_ocp_version"

                # Add group-specific PLR identifier to build tags
                group_build_tags=$(echo "${build_tags}" | jq --arg tag "$group_target_tag" '. + [$tag]')
            else
                echo "INFO: No target index for OCP group $group_ocp_version (staged release), skipping tag extraction"
                group_build_tags="$build_tags"
            fi
            echo "INFO: Group build tags for OCP $group_ocp_version:" \
                "$(echo "${group_build_tags}" | jq -r '.[]' | tr '\n' ' ')"

            # Initialize group-specific tracking for batch chaining
            local group_current_from_index="$group_from_index"
            local group_latest_iib_index_image=""
            local group_failed_batches=()
            local group_successful_batches=()

            # Calculate batches for this group
            local num_components
            num_components=$(echo "$group_components" | jq 'length')
            local group_num_batches=$(( (num_components + MAX_BATCH_SIZE - 1) / MAX_BATCH_SIZE ))

            echo "INFO: Creating $group_num_batches batch(es) for $num_components components" \
                "in OCP group $group_ocp_version"

            # Determine fromIndex for batch chaining
            get_current_from_index() {
                if [ "${mustOverwriteFromIndexImage}" = "true" ]; then
                    # IIB overwrites fromIndex - keep using original consistently
                    printf '%s' "${group_from_index}"
                else
                    # IIB creates new index - we MUST track what was actually created
                    if [ ${#group_successful_batches[@]} -eq 0 ]; then
                        # No successful batches yet - start from original fromIndex
                        printf '%s' "${group_from_index}"
                    else
                        # We have successful batches - MUST use latest IIB output
                        if [ -z "${group_latest_iib_index_image}" ]; then
                            echo "FATAL ERROR: We have successful batches but group_latest_iib_index_image is empty!"
                            echo "This would cause data loss. Group successful batches: ${group_successful_batches[*]}"
                            exit 1
                        fi
                        echo "${group_latest_iib_index_image}"
                    fi
                fi
            }

            # Get batch fragments
            get_batch_fragments() {
                local batch_num="$1"
                local start_idx=$((batch_num * MAX_BATCH_SIZE))
                local end_idx=$(((batch_num + 1) * MAX_BATCH_SIZE))
                local group_length
                group_length=$(echo "$group_components" | jq 'length')
                if [ "$end_idx" -gt "$group_length" ]; then
                    end_idx=$group_length
                fi

                echo "$group_components" | jq -c ".[${start_idx}:${end_idx}] | map(.containerImage)"
            }

            # Execute batch with validation
            execute_batch() {
                local batch_num="$1"
                local from_index="$2"

                echo "Executing group batch $((batch_num + 1)): fromIndex=${from_index}" \
                    "(OCP: $group_ocp_version)"

                # Get batch fragments
                local batch_fragments
                batch_fragments=$(get_batch_fragments "$batch_num")

                # Create InternalRequest for this batch
                internal-request --pipeline "update-fbc-catalog" \
                    -p fromIndex="${from_index}" \
                    -p fbcFragments="$(printf '%s' "${batch_fragments}" | jq -c .)" \
                    -p iibServiceAccountSecret="${iib_service_account_secret}" \
                    -p publishingCredentials="${publishing_credentials}" \
                    -p buildTimeoutSeconds="${build_timeout_seconds}" \
                    -p buildTags="$(jq -c . <<< "${group_build_tags}")" \
                    -p addArches="$(jq -c . <<< "${add_arches}")" \
                    -p mustPublishIndexImage="${mustPublishIndexImage}" \
                    -p mustOverwriteFromIndexImage="${mustOverwriteFromIndexImage}" \
                    -p taskGitUrl="$(params.taskGitUrl)" \
                    -p taskGitRevision="$(params.taskGitRevision)" \
                    --service-account "${internal_request_service_account}" \
                    -l ${TASK_LABEL}="${TASK_ID}" \
                    -l ${PIPELINERUN_LABEL}="$(params.pipelineRunUid)" \
                    --pipeline-timeout "${pipeline_timeout}" \
                    --task-timeout "${task_timeout}" \
                    -t "${request_timeout_seconds}" \
                    | tee "$(params.dataDir)/ir-$(context.taskRun.uid)-batch-$((batch_num + 1))-output.log"

                # Extract request name
                local internalRequest
                internalRequest=$(awk -F"'" '/created/ { print $2 }' \
                    "$(params.dataDir)/ir-$(context.taskRun.uid)-batch-$((batch_num + 1))-output.log")

                # Validate internal request succeeded
                local request_status
                request_status=$(kubectl get internalrequest "${internalRequest}" \
                    -o jsonpath='{.status.conditions[?(@.type=="Succeeded")].status}')

                if [ "${request_status}" != "True" ]; then
                    echo "ERROR: Batch $((batch_num + 1)) internal request failed"
                    local request_reason
                    request_reason=$(kubectl get internalrequest "${internalRequest}" \
                        -o jsonpath='{.status.conditions[?(@.type=="Succeeded")].reason}')
                    local request_message
                    request_message=$(kubectl get internalrequest "${internalRequest}" \
                        -o jsonpath='{.status.conditions[?(@.type=="Succeeded")].message}')
                    echo "Reason: ${request_reason}"
                    echo "Message: ${request_message}"
                    return 1
                fi

                # Extract and validate results
                local results
                results=$(kubectl get internalrequest "${internalRequest}" -o jsonpath='{.status.results}')

                if [ -z "${results}" ] || [ "${results}" = "{}" ]; then
                    echo "ERROR: Batch $((batch_num + 1)) succeeded but returned empty results"
                    return 1
                fi

                # Store results for this batch
                echo "${results}" > "$(params.dataDir)/ir-$(context.taskRun.uid)-batch-$((batch_num + 1))-result.json"

                # Process results for each fragment in this batch
                local decompressed_json_build_info
                decompressed_json_build_info="$(jq -r '.jsonBuildInfo' <<< "${results}" | base64 -d | gunzip)"
                local completion_time_raw
                completion_time_raw="$(jq -r '.updated' <<< "${decompressed_json_build_info}")"
                local completion_time
                completion_time=$(date +"${timestamp_format}" -d "${completion_time_raw}")

                # Process fragments in batch
                local fragment_index=0
                echo "$batch_fragments" | jq -r '.[]' | while read -r fragment; do
                    echo "Processing result for fragment: $fragment" \
                        "(group batch $((batch_num + 1)), OCP: $group_ocp_version)"

                    # Build individual component result
                    local build_results
                    build_results=$(jq \
                      --arg fragment "$fragment" \
                      --arg target_index "$group_target_index" \
                      --arg ocp_version "$group_ocp_version" \
                      --arg completion_time "$completion_time" \
                      --argjson decompressed_json "${decompressed_json_build_info}" \
                    '{
                      "fbc_fragment": $fragment,
                      "target_index": $target_index,
                      "ocp_version": $ocp_version,
                      "image_digests": (.indexImageDigests | split(" ") | del(.[] | select(. == ""))),
                      "index_image": $decompressed_json.index_image,
                      "index_image_resolved": $decompressed_json.index_image_resolved,
                      "completion_time": $completion_time,
                      "iibLog": .iibLog
                    }' <<< "${results}")

                    # Add to results file
                    export build_results
                    yq -i '.components += [ env(build_results) ]' "$RESULTS_FILE"

                    fragment_index=$((fragment_index + 1))
                done

                echo "Group batch $((batch_num + 1)) completed successfully" \
                    "for OCP $group_ocp_version"
                return 0
            }

            # Execute batches with failure tracking and retry logic
            for((batch_num=0; batch_num<"$group_num_batches"; batch_num++)); do
                echo "Processing group batch $((batch_num + 1))/${group_num_batches}" \
                    "for OCP $group_ocp_version..."

                group_current_from_index=$(get_current_from_index)

                  if execute_batch "$batch_num" "$group_current_from_index"; then
                    group_successful_batches+=("$batch_num")
                    echo "Group batch $((batch_num + 1)) succeeded for OCP $group_ocp_version"

                    # Extract latest IIB index image for next batch (only when not overwriting)
                    if [ "${mustOverwriteFromIndexImage}" = "false" ]; then
                        result_file="$(params.dataDir)/ir-$(context.taskRun.uid)-batch-$((batch_num + 1))-result.json"
                        batch_results=$(cat "${result_file}")
                        group_latest_iib_index_image=$(jq -r '.jsonBuildInfo' <<< "${batch_results}" | \
                            base64 -d | gunzip | jq -r '.index_image')
                        echo "Updated fromIndex for next batch: ${group_latest_iib_index_image}"
                    fi
                else
                    group_failed_batches+=("$batch_num")
                    echo "Group batch $((batch_num + 1)) failed, will retry later for OCP $group_ocp_version"
                    # Continue with next batch - no dependency blocking
                fi
            done

            # Retry failed batches (order doesn't matter)
            local max_retries
            max_retries=$(params.maxRetries)
            local batch_retry_delay
            batch_retry_delay=$(params.batchRetryDelaySeconds)

            for retry_attempt in $(seq 1 "$max_retries"); do
                if [ ${#group_failed_batches[@]} -eq 0 ]; then
                    echo "All group batches completed successfully for OCP $group_ocp_version"
                    break
                fi

                echo "Retry attempt ${retry_attempt}: ${#group_failed_batches[@]} group batches" \
                    "to retry for OCP $group_ocp_version"
                local still_failed=()

                for batch_num in "${group_failed_batches[@]}"; do
                    group_current_from_index=$(get_current_from_index)

                    if execute_batch "$batch_num" "$group_current_from_index" \
                        "$group_target_index"; then
                        echo "Group batch $((batch_num + 1)) succeeded on retry attempt ${retry_attempt}" \
                            "for OCP $group_ocp_version"

                        # Add to successful batches and extract latest IIB index image (only when not overwriting)
                        group_successful_batches+=("$batch_num")
                        if [ "${mustOverwriteFromIndexImage}" = "false" ]; then
                            result_file="$(params.dataDir)/ir-$(context.taskRun.uid)" \
                                "-batch-$((batch_num + 1))-result.json"
                            batch_results=$(cat "${result_file}")
                            group_latest_iib_index_image=$(jq -r '.jsonBuildInfo' <<< "${batch_results}" | \
                                base64 -d | gunzip | jq -r '.index_image')
                            echo "Updated fromIndex for next batch: ${group_latest_iib_index_image}"
                        fi
                    else
                        still_failed+=("$batch_num")
                        echo "Group batch $((batch_num + 1)) failed retry attempt ${retry_attempt}" \
                            "for OCP $group_ocp_version"
                    fi
                done

                group_failed_batches=("${still_failed[@]}")

                if [ ${#group_failed_batches[@]} -gt 0 ] && [ "${retry_attempt}" -lt "${max_retries}" ]; then
                    echo "Waiting ${batch_retry_delay} seconds before next retry attempt..."
                    sleep "${batch_retry_delay}"
                fi
            done

            # Final validation to ensure all components were processed
            if [ ${#group_failed_batches[@]} -gt 0 ]; then
                echo "ERROR: ${#group_failed_batches[@]} group batches failed after all retries" \
                    "for OCP $group_ocp_version:"
                for batch_num in "${group_failed_batches[@]}"; do
                    echo "  - Group batch $((batch_num + 1))"
                done
                echo "Cannot proceed with partial index for OCP $group_ocp_version -" \
                    "this would result in incomplete fragment coverage"
                exit 1
            fi

            echo "SUCCESS: All ${group_num_batches} group batches completed" \
                "successfully for OCP $group_ocp_version"
        }

        # Execute the multi-OCP processing
        process_all_ocp_groups

        # Get the final batch result for legacy compatibility (use last successful batch from any group)
        # Find the most recent batch result file
        latest_result_file=$(find "$(params.dataDir)" -name "ir-*-batch-*-result.json" \
            -type f -exec ls -t {} + 2>/dev/null | head -1)
        if [[ -n "$latest_result_file" ]]; then
            results=$(cat "$latest_result_file")
            # Extract batch number from filename for output log
            batch_number=$(basename "$latest_result_file" | sed 's/.*-batch-\([0-9]*\)-result.json/\1/')
            internalRequest=$(awk -F"'" '/created/ { print $2 }' \
                "$(params.dataDir)/ir-$(context.taskRun.uid)-batch-${batch_number}-output.log")
        else
            echo "ERROR: No batch result files found"
            exit 1
        fi

        echo "Final publishing and compatibility values:"
        echo "  - mustPublishIndexImage: ${mustPublishIndexImage}"

        # Store the results in Tekton's results files

        # Store deprecated results for compatibility
        conditions=$(kubectl get internalrequest "${internalRequest}" \
          -o jsonpath='{.status.conditions[?(@.type=="Succeeded")]}')
        jq '.reason // "Unset"'  <<< "${conditions}" | tee "$(results.requestReason.path)"
        jq '.message // "Unset"' <<< "${conditions}" | tee "$(results.requestMessage.path)"
        jq -r '.indexImageDigests' <<< "${results}" |  tee "$(results.indexImageDigests.path)"

        # Output batch logs for debugging
        jq -r '.iibLog' <<< "${results}"
        RC="$(jq -r '.exitCode' <<< "${results}")"

        # Summarize batch results
        if [ "$mustPublishIndexImage" = "true" ]; then
          echo "Index image will be published."
        else
          echo "Index image will not be published (decision made by prepare-fbc-parameters)."
        fi

        if [ "$RC" -ne 0 ]; then
          echo "Batch processing failed, check the batch logs above to understand the reason"
          exit "$RC"
        fi
        
        # Calculate total components processed across all OCP groups
        total_components=$(jq '.components | length' "$SNAPSHOT_PATH")
        echo "Multi-OCP batch processing completed successfully with $total_components components" \
            "across $(echo "$ocp_versions" | wc -w) OCP versions"
        echo "Results file:"
        cat "$RESULTS_FILE"
    - name: create-trusted-artifact
      computeResources:
        limits:
          memory: 128Mi
        requests:
          memory: 128Mi
          cpu: 250m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/create-trusted-artifact/create-trusted-artifact.yaml
      params:
        - name: ociStorage
          value: $(params.ociStorage)
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(results.sourceDataArtifact.path)
