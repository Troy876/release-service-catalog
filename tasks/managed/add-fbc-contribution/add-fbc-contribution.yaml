---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: add-fbc-contribution
  annotations:
    tekton.dev/pipelines.minVersion: "0.12.1"
    tekton.dev/tags: release
spec:
  description: |-
    Task to create an internalrequest to add fbc contributions to index images
  params:
    - name: snapshotPath
      description: Path to the JSON string of the mapped Snapshot spec in the data workspace
      type: string
    - name: dataPath
      description: Path to the JSON string of the merged data to use in the data workspace
      type: string
    - name: fromIndex
      type: string
      description: fromIndex value updated by update-ocp-tag task
    - name: targetIndex
      type: string
      description: targetIndex value updated by update-ocp-tag task
    - name: pipelineRunUid
      type: string
      description: The uid of the current pipelineRun. Used as a label value when creating internal requests
    - name: resultsDirPath
      type: string
      description: Path to the results directory in the data workspace
    - name: ociStorage
      description: The OCI repository where the Trusted Artifacts are stored
      type: string
      default: "empty"
    - name: ociArtifactExpiresAfter
      description: Expiration date for the trusted artifacts created in the
        OCI repository. An empty string means the artifacts do not expire
      type: string
      default: "1d"
    - name: trustedArtifactsDebug
      description: Flag to enable debug logging in trusted artifacts. Set to a non-empty string to enable
      type: string
      default: ""
    - name: orasOptions
      description: oras options to pass to Trusted Artifacts calls
      type: string
      default: ""
    - name: sourceDataArtifact
      type: string
      description: Location of trusted artifacts to be used to populate data directory
      default: ""
    - name: dataDir
      description: The location where data will be stored
      type: string
      default: /var/workdir/release
    - name: taskGitUrl
      type: string
      description: The url to the git repo where the release-service-catalog tasks and stepactions to be used are stored
    - name: taskGitRevision
      type: string
      description: The revision in the taskGitUrl repo to be used
  results:
    - name: buildTimestamp
      description: Build timestamp used in the tag
    - name: mustSignIndexImage
      description: Whether the index image should be signed
    - name: mustPublishIndexImage
      description: Whether the index image should be published
    - name: isFbcOptIn
      description: Indicates whether the FBC fragment is opt-in (true/false)
    - name: requestTargetIndex
      description: The targetIndex used in this request
    - name: requestResultsFile
      description: Internal Request results file
    - name: internalRequestResultsFile
      description: Additional Internal Request results file
    - name: requestMessage
      description: Internal Request message
    - name: requestReason
      description: Internal Request reason
    - name: indexImageDigests
      description: list of manifest digests for each arch from manifest list in index image
    - name: sourceDataArtifact
      type: string
      description: Produced trusted data artifact
  volumes:
    - name: workdir
      emptyDir: {}
  stepTemplate:
    volumeMounts:
      - mountPath: /var/workdir
        name: workdir
    env:
      - name: IMAGE_EXPIRES_AFTER
        value: $(params.ociArtifactExpiresAfter)
      - name: "ORAS_OPTIONS"
        value: "$(params.orasOptions)"
      - name: "DEBUG"
        value: "$(params.trustedArtifactsDebug)"
  steps:
    - name: use-trusted-artifact
      computeResources:
        limits:
          memory: 64Mi
        requests:
          memory: 64Mi
          cpu: 30m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/use-trusted-artifact/use-trusted-artifact.yaml
      params:
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(params.sourceDataArtifact)
    - name: add-contribution
      image: quay.io/konflux-ci/release-service-utils:0f82be4be43294b6a96846d87ef7f7c0b9e34267
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi  # was exiting with code 137 when set to 256Mi
          cpu: 200m
      script: |
        #!/usr/bin/env bash
        #
        set -eo pipefail

        SNAPSHOT_PATH="$(params.dataDir)/$(params.snapshotPath)"
        DATA_FILE="$(params.dataDir)/$(params.dataPath)"
        if [ ! -f "${DATA_FILE}" ] ; then
            echo "No valid data file was provided."
            exit 1
        fi

        # adding a new result as modifying the current one used breaks e2e for single component.
        # to be handled in RELEASE-1640.
        RESULTS_FILE="$(params.resultsDirPath)/internal-requests-results.json"
        echo -n "$RESULTS_FILE" > "$(results.internalRequestResultsFile.path)"
        RESULTS_FILE="$(params.dataDir)/$(params.resultsDirPath)/internal-requests-results.json"

        echo -n "$(params.dataDir)/$(params.pipelineRunUid)/ir-$(context.taskRun.uid)-result.json" \
          > "$(results.requestResultsFile.path)"

        default_build_timeout_seconds="3600"
        default_request_timeout_seconds="3600"

        hotfix=$(jq -r '.fbc.hotfix // "false"' "${DATA_FILE}")
        pre_ga=$(jq -r '.fbc.preGA // "false"' "${DATA_FILE}")
        staged_index=$(jq -r '.fbc.stagedIndex // "false"' "${DATA_FILE}")
        product_name=$(jq -r '.fbc.productName // ""' "${DATA_FILE}")
        product_version=$(jq -r '.fbc.productVersion // ""' "${DATA_FILE}")
        build_timeout_seconds=$(jq -r --arg build_timeout_seconds ${default_build_timeout_seconds} \
            '.fbc.buildTimeoutSeconds // $build_timeout_seconds' "${DATA_FILE}")
        request_timeout_seconds=$(jq -r --arg request_timeout_seconds ${default_request_timeout_seconds} \
            '.fbc.requestTimeoutSeconds // $request_timeout_seconds' "${DATA_FILE}")
        internal_request_service_account=$(jq -r '.fbc.internalRequestServiceAccount // "release-service-account"' \
            "${DATA_FILE}")

        if [ "${staged_index}" = "true" ]; then
          iib_service_account_secret="iib-service-account-stage"
        else
          iib_service_account_secret="iib-service-account-prod"
        fi
        publishing_credentials=$(jq -r '.fbc.publishingCredentials // "catalog-publishing-secret"' "$DATA_FILE")

        timestamp_format=$(jq -r '.fbc.timestampFormat // "%s"' "${DATA_FILE}")
        timestamp=$(date "+${timestamp_format}")

        # default target_index
        target_index=$(params.targetIndex)

        if [ "${hotfix}" = "true" ] && [ "${pre_ga}" = "true" ]; then
          echo "fbc.preGA and fbc.hotfix are mutually exclusive. Please set just one in the ReleasePlanAdmission"
          exit 1
        fi

        # the target_index is modified when the pipelinerun is a for `hotfix` or a `pre-GA` release
        if [ "${hotfix}" = "true" ]; then
          issue_id=$(jq -r '.fbc.issueId // empty' "${DATA_FILE}")
          if [ -z "${issue_id}" ]; then
            echo "Hotfix releases requires the issue id set in the 'fbc.issueId' key of the ReleasePlanAdmission " \
                 "spec.data field"
            exit 1
          fi
          target_index="${target_index}-${issue_id}-${timestamp}"
        elif [ "${pre_ga}" = "true" ]; then
          if [ -z "${product_name}" ] || [ -z "${product_version}" ]; then
            echo "Pre-GA releases require 'fbc.productName' and 'fbc.productVersion' set in the ReleasePlanAdmission " \
                 "spec.data field"
            exit 1
          fi
          target_index="${target_index}-${product_name}-${product_version}-${timestamp}"
        fi

        # to keep compatibility with current single component mode
        echo -n "$timestamp" > "$(results.buildTimestamp.path)"
        echo -n "$target_index" > "$(results.requestTargetIndex.path)"
        jq -n --arg target_index "$target_index" \
          '{"index_image": {"target_index": $target_index}, "components": []}' \
          | tee "$RESULTS_FILE"

        pipelinerun_label="internal-services.appstudio.openshift.io/pipelinerun-uid"

        # Create component groups with enriched data for batching
        LENGTH=$(jq -r '.components | length' "${SNAPSHOT_PATH}")
        echo "Creating component groups for batching..."
        
        # Build enriched components with resolved target_index
        enriched_components='[]'
        # Read global buildTags and addArches from data file
        build_tags=$(jq '.fbc.buildTags // []' "${DATA_FILE}")
        add_arches=$(jq '.fbc.addArches // []' "${DATA_FILE}")
        
        for((i=0; i<LENGTH; i++)); do
          ocp_version=$(jq -cr --argjson i "$i" '.components[$i].ocpVersion' "${SNAPSHOT_PATH}")
          fbc_fragment=$(jq -cr --argjson i "$i" '.components[$i].containerImage' "${SNAPSHOT_PATH}")
          from_index=$(jq -cr --argjson i "$i" '.components[$i].updatedFromIndex' "${SNAPSHOT_PATH}")
          base_target_index=$(jq -cr --argjson i "$i" '.components[$i].updatedTargetIndex' "${SNAPSHOT_PATH}")

          # Apply same target_index resolution logic as before
          resolved_target_index="$base_target_index"
          if [ "${hotfix}" = "true" ]; then
            issue_id=$(jq -r '.fbc.issueId // empty' "${DATA_FILE}")
            if [ -z "${issue_id}" ]; then
              echo "Hotfix releases requires the issue id set in the 'fbc.issueId' key of the ReleasePlanAdmission " \
                   "spec.data field"
              exit 1
            fi
            resolved_target_index="${base_target_index}-${issue_id}-${timestamp}"
          elif [ "${pre_ga}" = "true" ]; then
            if [ -z "${product_name}" ] || [ -z "${product_version}" ]; then
              echo "Pre-GA releases require 'fbc.productName' and 'fbc.productVersion' set in the " \
                   "ReleasePlanAdmission spec.data field"
              exit 1
            fi
            resolved_target_index="${base_target_index}-${product_name}-${product_version}-${timestamp}"
          fi

          # Add enriched component to array
          enriched_component=$(jq -n \
            --arg ocpVersion "$ocp_version" \
            --arg fbcFragment "$fbc_fragment" \
            --arg fromIndex "$from_index" \
            --arg targetIndex "$resolved_target_index" \
            --argjson buildTags "$build_tags" \
            --argjson addArches "$add_arches" \
            '{
              ocpVersion: $ocpVersion,
              fbcFragment: $fbcFragment,
              fromIndex: $fromIndex,
              targetIndex: $targetIndex,
              buildTags: $buildTags,
              addArches: $addArches
            }')
          
          enriched_components=$(jq --argjson component "$enriched_component" \
            '. += [$component]' <<< "$enriched_components")
        done

        # Group components by (fromIndex, targetIndex, ocpVersion)
        # buildTags and addArches are global IIB parameters, not per-fragment
        echo "Grouping components by batching criteria..."
        groups=$(jq -r '
          group_by([.fromIndex, .targetIndex, .ocpVersion]) |
          to_entries |
          map({
            key: .key,
            group_id: (.key | tostring),
            fromIndex: .value[0].fromIndex,
            targetIndex: .value[0].targetIndex,
            ocpVersion: .value[0].ocpVersion,
            buildTags: $buildTags,
            addArches: $addArches,
            fragments: [.value[].fbcFragment],
            count: (.value | length)
          })
        ' --argjson buildTags "$build_tags" --argjson addArches "$add_arches" <<< "$enriched_components")

        echo "Created $(jq -r 'length' <<< "$groups") batch groups"
        
        # Process each group with batched InternalRequest
        group_count=$(jq -r 'length' <<< "$groups")
        for((g=0; g<group_count; g++)); do
          group=$(jq -r --argjson g "$g" '.[$g]' <<< "$groups")
          
          # Extract group parameters
          group_from_index=$(jq -r '.fromIndex' <<< "$group")
          group_target_index=$(jq -r '.targetIndex' <<< "$group")
          group_ocp_version=$(jq -r '.ocpVersion' <<< "$group")
          # buildTags and addArches are global, use values from data file
          group_fragments=$(jq '.fragments' <<< "$group")
          group_fragment_count=$(jq -r '.count' <<< "$group")
          
          echo "Processing batch group $((g+1))/$group_count:"
          echo "  - fromIndex: $group_from_index"
          echo "  - targetIndex: $group_target_index"
          echo "  - ocpVersion: $group_ocp_version"
          echo "  - fragments: $group_fragment_count items"
          
          # Calculate timeout for this batch
          finally_task_timeout=300
          pipeline_timeout=$(date -u "+%Hh%Mm%Ss" -d @$(( request_timeout_seconds + finally_task_timeout )))
          task_timeout=$(date -u "+%Hh%Mm%Ss" -d @$(( request_timeout_seconds )))

          # Create batched InternalRequest
          echo "Creating batched InternalRequest for $group_fragment_count fragments..."
          internal-request --pipeline "update-fbc-catalog" \
              -p fromIndex="${group_from_index}" \
              -p targetIndex="${group_target_index}" \
              -p fbcFragments="$(printf '%s' "${group_fragments}" | jq -c .)" \
              -p iibServiceAccountSecret="${iib_service_account_secret}" \
              -p publishingCredentials="${publishing_credentials}" \
              -p buildTimeoutSeconds="${build_timeout_seconds}" \
              -p buildTags="$(jq -c . <<< "${build_tags}")" \
              -p addArches="$(jq -c . <<< "${add_arches}")" \
              -p hotfix="${hotfix}" \
              -p stagedIndex="${staged_index}" \
              -p taskGitUrl="$(params.taskGitUrl)" \
              -p taskGitRevision="$(params.taskGitRevision)" \
              --service-account "${internal_request_service_account}" \
              -l ${pipelinerun_label}="$(params.pipelineRunUid)" \
              --pipeline-timeout "${pipeline_timeout}" \
              --task-timeout "${task_timeout}" \
              -t "${request_timeout_seconds}" |tee "$(params.dataDir)"/ir-"$(context.taskRun.uid)"-batch-${g}-output.log

          internalRequest=$(awk -F"'" '/created/ { print $2 }' \
            "$(params.dataDir)"/ir-"$(context.taskRun.uid)"-batch-${g}-output.log)
          echo "Batch request created: ${internalRequest}"

          # Fetch batched results
          results=$(kubectl get internalrequest "${internalRequest}" -o jsonpath='{.status.results}')
          echo "${results}" > "$(params.dataDir)/ir-$(context.taskRun.uid)-batch-${g}-result.json"

          completion_time_raw="$(jq -r '.jsonBuildInfo | fromjson | .updated' <<< "${results}")"
          completion_time=$(date +"${timestamp_format}" -d "${completion_time_raw}")

          # Process results for each fragment in the batch
          fragments_array=$(jq -r '.fragments' <<< "$group")
          fragment_count=$(jq -r 'length' <<< "$fragments_array")
          
          for((f=0; f<fragment_count; f++)); do
            fragment=$(jq -r --argjson f "$f" '.[$f]' <<< "$fragments_array")
            
            echo "Processing result for fragment: $fragment"
            
            # Build individual component result
            build_results=$(jq \
              --arg fragment "$fragment" \
              --arg target_index "$group_target_index" \
              --arg ocp_version "$group_ocp_version" \
              --arg completion_time "$completion_time" \
            '{
              "fbc_fragment": $fragment,
              "target_index": $target_index,
              "ocp_version": $ocp_version,
              "image_digests": (.indexImageDigests | split(" ") | del(.[] | select(. == ""))),
              "index_image": (.jsonBuildInfo | fromjson | .index_image),
              "index_image_resolved": (.jsonBuildInfo | fromjson | .index_image_resolved),
              "completion_time": $completion_time,
              "iibLog": .iibLog
            }' <<< "${results}")

            # Add to results file
            export build_results
            yq -i '.components += [ env(build_results) ]' "$RESULTS_FILE"
          done

          # Extract flags from batch results (same for all fragments in batch)
          mustSignIndexImage=$(jq -r '.genericResult | fromjson' <<< "${results}" \
            | jq -r '.sign_index_image' |tr -d "\n")
          mustPublishIndexImage=$(jq -r '.genericResult | fromjson' <<< "${results}" \
            | jq -r '.publish_index_image' |tr -d "\n")
          fbc_opt_in=$(jq -r '.genericResult | fromjson' <<< "${results}" | jq -cr '.fbc_opt_in')

          # Store the results in Tekton's results files (last batch wins for compatibility)
          echo -en "${mustPublishIndexImage}" | tee "$(results.mustPublishIndexImage.path)"
          echo -en "${mustSignIndexImage}" | tee "$(results.mustSignIndexImage.path)"
          echo -en "${fbc_opt_in}" | tee "$(results.isFbcOptIn.path)"

          # Store deprecated results for compatibility
          conditions=$(kubectl get internalrequest "${internalRequest}" \
            -o jsonpath='{.status.conditions[?(@.type=="Succeeded")]}')
          jq '.reason // "Unset"'  <<< "${conditions}" | tee "$(results.requestReason.path)"
          jq '.message // "Unset"' <<< "${conditions}" | tee "$(results.requestMessage.path)"
          jq -r '.indexImageDigests' <<< "${results}" |  tee "$(results.indexImageDigests.path)"

          # Output batch logs for debugging
          jq -r '.iibLog' <<< "${results}"
          RC="$(jq -r '.exitCode' <<< "${results}")"

          # Summarize batch results
          if [ "$mustPublishIndexImage" = "true" ]; then
            echo "Index image will be published for batch group $((g+1))."
          elif [ "$fbc_opt_in" = "false" ]; then
            echo "Index image will not be published for batch group $((g+1)) because" \
              "fbc_opt_in is set to false in Pyxis."
            echo "If this is the first time you are releasing, make sure you request fbc_opt_in in Pyxis."
          elif [ "${staged_index}" = "true" ]; then
            echo "Index image will not be published for batch group $((g+1)) because this is a staging release."
          else
            echo "Index image will not be published for batch group $((g+1)) for an unspecified reason."
          fi

          if [ "$RC" -ne 0 ]; then
            echo "Batch group $((g+1)) failed to be processed, check the batch logs above to understand the reason"
            exit "$RC"
          fi
          
          echo "Batch group $((g+1))/$group_count completed successfully"
        done
        
        echo "All batch groups processed successfully"

        # Consolidate flags from all batches - all batch results should match for signing and publishing
        echo "Consolidating flags across all batch groups..."
        all_batch_flags='[]'
        for((g=0; g<group_count; g++)); do
          batch_results_file="$(params.dataDir)/ir-$(context.taskRun.uid)-batch-${g}-result.json"
          if [ -f "$batch_results_file" ]; then
            batch_results=$(cat "$batch_results_file")
            batch_flags=$(jq -r '.genericResult | fromjson' <<< "${batch_results}")
            all_batch_flags=$(jq --argjson flags "$batch_flags" '. += [$flags]' <<< "$all_batch_flags")
          fi
        done

        # Check if all batches have the same flag values
        fbcPipelineFlags=$(jq '. | unique' <<< "$all_batch_flags")
        unique_flag_sets=$(jq -r 'length' <<< "$fbcPipelineFlags")
        
        echo "Found $unique_flag_sets unique flag sets across $group_count batch groups"
        
        if [ "$unique_flag_sets" == 1 ]; then
          echo "All batch groups have matching flags - enabling signing and publishing as appropriate"
          jq -r '.[0].sign_index_image' <<< "$fbcPipelineFlags" | tr -d "\n" \
            | tee "$(results.mustSignIndexImage.path)"
          jq -r '.[0].publish_index_image' <<< "$fbcPipelineFlags" | tr -d "\n" \
            | tee "$(results.mustPublishIndexImage.path)"
          jq -r '.[0].fbc_opt_in' <<< "$fbcPipelineFlags" | tr -d "\n" | tee "$(results.isFbcOptIn.path)"
        else
          echo "Batch groups have mismatched flags - disabling signing and publishing for safety"
          echo -en "false" | tee "$(results.mustSignIndexImage.path)"
          echo -en "false" | tee "$(results.mustPublishIndexImage.path)"
          echo -en "false" | tee "$(results.isFbcOptIn.path)"
        fi

        # if there are multiple components but they are for the same ocp version, only a single index_image needs
        # to be signed/published.
        VERSIONS_COUNT="$(jq -r '[ .components[].ocp_version ] | unique | length' "$RESULTS_FILE")"
        if [[ "$LENGTH" -gt 1 && "$VERSIONS_COUNT" == 1 ]]; then
          # keep only the most up to date build index_image (use completion_time since .updated may not exist)
          RESULTS_TEMP_FILE="/tmp/results-temp.json"
          jq -r '.components = [ .components | sort_by(.completion_time) | last ]' \
            "$RESULTS_FILE" > "$RESULTS_TEMP_FILE"
          mv "$RESULTS_TEMP_FILE" "$RESULTS_FILE"
        fi
        echo "Results file:"
        cat "$RESULTS_FILE"
    - name: create-trusted-artifact
      computeResources:
        limits:
          memory: 128Mi
        requests:
          memory: 128Mi
          cpu: 250m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/create-trusted-artifact/create-trusted-artifact.yaml
      params:
        - name: ociStorage
          value: $(params.ociStorage)
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(results.sourceDataArtifact.path)
