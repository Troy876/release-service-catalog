---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: add-fbc-contribution
  annotations:
    tekton.dev/pipelines.minVersion: "0.12.1"
    tekton.dev/tags: release
spec:
  description: |-
    Task to create an internalrequest to add fbc contributions to index images
  params:
    - name: snapshotPath
      description: Path to the JSON string of the mapped Snapshot spec in the data workspace
      type: string
    - name: dataPath
      description: Path to the JSON string of the merged data to use in the data workspace
      type: string
    - name: fromIndex
      type: string
      description: fromIndex value updated by update-ocp-tag task
    - name: targetIndex
      type: string
      description: targetIndex value updated by update-ocp-tag task
    - name: pipelineRunUid
      type: string
      description: The uid of the current pipelineRun. Used as a label value when creating internal requests
    - name: ocpVersion
      type: string
      description: The OCP version for all components in this release
    - name: resultsDirPath
      type: string
      description: Path to the results directory in the data workspace
    - name: ociStorage
      description: The OCI repository where the Trusted Artifacts are stored
      type: string
      default: "empty"
    - name: ociArtifactExpiresAfter
      description: Expiration date for the trusted artifacts created in the
        OCI repository. An empty string means the artifacts do not expire
      type: string
      default: "1d"
    - name: trustedArtifactsDebug
      description: Flag to enable debug logging in trusted artifacts. Set to a non-empty string to enable
      type: string
      default: ""
    - name: orasOptions
      description: oras options to pass to Trusted Artifacts calls
      type: string
      default: ""
    - name: sourceDataArtifact
      type: string
      description: Location of trusted artifacts to be used to populate data directory
      default: ""
    - name: dataDir
      description: The location where data will be stored
      type: string
      default: /var/workdir/release
    - name: taskGitUrl
      type: string
      description: The url to the git repo where the release-service-catalog tasks and stepactions to be used are stored
    - name: taskGitRevision
      type: string
      description: The revision in the taskGitUrl repo to be used
    - name: maxBatchSize
      type: string
      description: Maximum number of FBC fragments to process in a single batch
      default: "5"
  results:
    - name: buildTimestamp
      description: Build timestamp used in the tag
    - name: mustSignIndexImage
      description: Whether the index image should be signed
    - name: mustPublishIndexImage
      description: Whether the index image should be published
    - name: isFbcOptIn
      description: Indicates whether the FBC fragment is opt-in (true/false)
    - name: requestTargetIndex
      description: The targetIndex used in this request
    - name: requestResultsFile
      description: Internal Request results file
    - name: internalRequestResultsFile
      description: Additional Internal Request results file
    - name: requestMessage
      description: Internal Request message
    - name: requestReason
      description: Internal Request reason
    - name: indexImageDigests
      description: list of manifest digests for each arch from manifest list in index image
    - name: sourceDataArtifact
      type: string
      description: Produced trusted data artifact
  volumes:
    - name: workdir
      emptyDir: {}
  stepTemplate:
    volumeMounts:
      - mountPath: /var/workdir
        name: workdir
    env:
      - name: IMAGE_EXPIRES_AFTER
        value: $(params.ociArtifactExpiresAfter)
      - name: "ORAS_OPTIONS"
        value: "$(params.orasOptions)"
      - name: "DEBUG"
        value: "$(params.trustedArtifactsDebug)"
  steps:
    - name: use-trusted-artifact
      computeResources:
        limits:
          memory: 64Mi
        requests:
          memory: 64Mi
          cpu: 30m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/use-trusted-artifact/use-trusted-artifact.yaml
      params:
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(params.sourceDataArtifact)
    - name: add-contribution
      image: quay.io/konflux-ci/release-service-utils:0f82be4be43294b6a96846d87ef7f7c0b9e34267
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi  # was exiting with code 137 when set to 256Mi
          cpu: 200m
      script: |
        #!/usr/bin/env bash
        #
        set -eo pipefail

        SNAPSHOT_PATH="$(params.dataDir)/$(params.snapshotPath)"
        DATA_FILE="$(params.dataDir)/$(params.dataPath)"
        if [ ! -f "${DATA_FILE}" ] ; then
            echo "No valid data file was provided."
            exit 1
        fi

        # adding a new result as modifying the current one used breaks e2e for single component.
        # to be handled in RELEASE-1640.
        RESULTS_FILE="$(params.resultsDirPath)/internal-requests-results.json"
        echo -n "$RESULTS_FILE" > "$(results.internalRequestResultsFile.path)"
        RESULTS_FILE="$(params.dataDir)/$(params.resultsDirPath)/internal-requests-results.json"

        echo -n "$(params.dataDir)/$(params.pipelineRunUid)/ir-$(context.taskRun.uid)-result.json" \
          > "$(results.requestResultsFile.path)"

        default_build_timeout_seconds="3600"
        default_request_timeout_seconds="3600"

        hotfix=$(jq -r '.fbc.hotfix // "false"' "${DATA_FILE}")
        pre_ga=$(jq -r '.fbc.preGA // "false"' "${DATA_FILE}")
        staged_index=$(jq -r '.fbc.stagedIndex // "false"' "${DATA_FILE}")
        product_name=$(jq -r '.fbc.productName // ""' "${DATA_FILE}")
        product_version=$(jq -r '.fbc.productVersion // ""' "${DATA_FILE}")
        build_timeout_seconds=$(jq -r --arg build_timeout_seconds ${default_build_timeout_seconds} \
            '.fbc.buildTimeoutSeconds // $build_timeout_seconds' "${DATA_FILE}")
        request_timeout_seconds=$(jq -r --arg request_timeout_seconds ${default_request_timeout_seconds} \
            '.fbc.requestTimeoutSeconds // $request_timeout_seconds' "${DATA_FILE}")
        internal_request_service_account=$(jq -r '.fbc.internalRequestServiceAccount // "release-service-account"' \
            "${DATA_FILE}")

        if [ "${staged_index}" = "true" ]; then
          iib_service_account_secret="iib-service-account-stage"
        else
          iib_service_account_secret="iib-service-account-prod"
        fi
        publishing_credentials=$(jq -r '.fbc.publishingCredentials // "catalog-publishing-secret"' "$DATA_FILE")

        timestamp_format=$(jq -r '.fbc.timestampFormat // "%s"' "${DATA_FILE}")
        timestamp=$(date "+${timestamp_format}")

        # Tag validation and sanitization function
        validate_and_sanitize_tag_component() {
          local component="$1"
          local component_name="$2"
          local max_length="$3"
          
          # Validate not empty
          if [ -z "$component" ]; then
            echo "ERROR: ${component_name} cannot be empty"
            exit 1
          fi
          
          # Sanitize invalid characters by replacing with hyphens
          sanitized="${component//[^a-zA-Z0-9._-]/-}"
          
          # Collapse consecutive special characters to single hyphen
          while [[ "$sanitized" =~ [._-][._-] ]]; do
            sanitized="${sanitized//[._-][._-]/-}"
          done
          
          # Remove leading and trailing special characters
          sanitized=$(echo "$sanitized" | sed 's/^[-._]*//;s/[-._]*$//')
          
          # Check if sanitization resulted in empty string
          if [ -z "$sanitized" ]; then
            echo "ERROR: ${component_name} '$component' sanitization resulted in empty string"
            exit 1
          fi
          
          # Check for reserved names
          case "$sanitized" in
            "latest"|"main"|"master"|"HEAD")
              echo "ERROR: ${component_name} cannot use reserved name: '$sanitized' (from '$component')"
              exit 1
              ;;
          esac
          
          # Report sanitization if changes were made
          if [ "$component" != "$sanitized" ]; then
            echo "INFO: ${component_name} sanitized from '$component' to '$sanitized'"
          fi
          
          # Check length and truncate if necessary
          if [ ${#sanitized} -gt "$max_length" ]; then
            truncated=$(echo "$sanitized" | cut -c1-"$max_length")
            # Ensure truncated version doesn't end with special char
            truncated="${truncated%[._-]}"
            while [[ "$truncated" =~ [._-]$ ]]; do
              truncated="${truncated%[._-]}"
            done
            echo "WARNING: ${component_name} truncated from '$sanitized' to '$truncated' (max ${max_length} chars)"
            echo "$truncated"
          else
            echo "$sanitized"
          fi
        }

        # default target_index
        target_index=$(params.targetIndex)

        # Calculate tag length limits - will be set per release type
        max_tag_length=128
        max_product_name_length=0
        max_product_version_length=0

        if [ "${hotfix}" = "true" ] && [ "${pre_ga}" = "true" ]; then
          echo "fbc.preGA and fbc.hotfix are mutually exclusive. Please set just one in the ReleasePlanAdmission"
          exit 1
        fi

        # the target_index is modified when the pipelinerun is a for `hotfix` or a `pre-GA` release
        if [ "${hotfix}" = "true" ]; then
          issue_id=$(jq -r '.fbc.issueId // empty' "${DATA_FILE}")
          if [ -z "${issue_id}" ]; then
            echo "Hotfix releases requires the issue id set in the 'fbc.issueId' key of the ReleasePlanAdmission " \
                 "spec.data field"
            exit 1
          fi
          target_index="${target_index}-${issue_id}-${timestamp}"
        elif [ "${pre_ga}" = "true" ]; then
          if [ -z "${product_name}" ] || [ -z "${product_version}" ]; then
            echo "Pre-GA releases require 'fbc.productName' and 'fbc.productVersion' set in the ReleasePlanAdmission " \
                 "spec.data field"
            exit 1
          fi
          
          # Sanitize product version first (assume shorter, max 20 chars)
          max_product_version_length=20
          sanitized_product_version=$(validate_and_sanitize_tag_component "$product_version" \
                                              "fbc.productVersion" "$max_product_version_length")
          
          # Calculate remaining space for product name after all other components
          # Format: ${target_index}-${product_name}-${sanitized_product_version}-${timestamp}
          base_target_length=${#target_index}
          version_length=${#sanitized_product_version}
          timestamp_length=${#timestamp}
          separator_chars=3  # Three hyphens
          reserved_space=$((base_target_length + version_length + timestamp_length + separator_chars))
          available_space=$((max_tag_length - reserved_space))

          # Check if we have minimum space for product name
          min_product_name_length=8
          if [ $available_space -lt $min_product_name_length ]; then
            echo "ERROR: Not enough space for product name in pre-GA tag. " \
                 "Available: ${available_space}, needed: ${min_product_name_length}"
            echo "Base components use ${reserved_space} of ${max_tag_length} characters"
            exit 1
          fi

          # Use all remaining space for product name
          max_product_name_length=$available_space
          sanitized_product_name=$(validate_and_sanitize_tag_component "$product_name" \
                                           "fbc.productName" "$max_product_name_length")
          
          target_index="${target_index}-${sanitized_product_name}-${sanitized_product_version}-${timestamp}"
        fi

        # to keep compatibility with current single component mode
        echo -n "$timestamp" > "$(results.buildTimestamp.path)"
        echo -n "$target_index" > "$(results.requestTargetIndex.path)"
        jq -n --arg target_index "$target_index" \
          '{"index_image": {"target_index": $target_index}, "components": []}' \
          | tee "$RESULTS_FILE"

        pipelinerun_label="internal-services.appstudio.openshift.io/pipelinerun-uid"

        # Create batches with size limits
        LENGTH=$(jq -r '.components | length' "${SNAPSHOT_PATH}")
        MAX_BATCH_SIZE="$(params.maxBatchSize)"
        echo "Processing $LENGTH components with maximum batch size of $MAX_BATCH_SIZE..."

        # Read global buildTags and addArches from data file
        build_tags=$(jq '.fbc.buildTags // []' "${DATA_FILE}")
        add_arches=$(jq '.fbc.addArches // []' "${DATA_FILE}")

        # Apply target_index resolution logic for hotfix/pre-GA
        resolved_target_index="$(params.targetIndex)"  # Use original targetIndex parameter
        if [ "${hotfix}" = "true" ]; then
          issue_id=$(jq -r '.fbc.issueId // empty' "${DATA_FILE}")
          if [ -z "${issue_id}" ]; then
            echo "Hotfix releases requires the issue id set in the 'fbc.issueId' key of the ReleasePlanAdmission " \
                 "spec.data field"
            exit 1
          fi
          resolved_target_index="${resolved_target_index}-${issue_id}-${timestamp}"
        elif [ "${pre_ga}" = "true" ]; then
          if [ -z "${product_name}" ] || [ -z "${product_version}" ]; then
            echo "Pre-GA releases require 'fbc.productName' and 'fbc.productVersion' set in the " \
                 "ReleasePlanAdmission spec.data field"
            exit 1
          fi
          # Reuse already sanitized values from above
          resolved_target_index="${resolved_target_index}-${sanitized_product_name}-${sanitized_product_version}-${timestamp}"
        fi

        # Calculate number of batches needed
        NUM_BATCHES=$(( (LENGTH + MAX_BATCH_SIZE - 1) / MAX_BATCH_SIZE ))
        echo "Creating $NUM_BATCHES batch(es) for $LENGTH components"

        # Common values for all batches
        batch_from_index="$(params.fromIndex)"
        batch_target_index="$resolved_target_index"
        batch_ocp_version="$(params.ocpVersion)"
        
        # Calculate timeout for batches
        finally_task_timeout=300
        pipeline_timeout=$(date -u "+%Hh%Mm%Ss" -d @$(( request_timeout_seconds + finally_task_timeout )))
        task_timeout=$(date -u "+%Hh%Mm%Ss" -d @$(( request_timeout_seconds )))

        # Initialize the current fromIndex for chaining batches
        current_from_index="$batch_from_index"
        
        # Process each batch sequentially, chaining index images
        for((batch_num=0; batch_num<NUM_BATCHES; batch_num++)); do
          start_idx=$((batch_num * MAX_BATCH_SIZE))
          end_idx=$(((batch_num + 1) * MAX_BATCH_SIZE))
          if [ $end_idx -gt "$LENGTH" ]; then
            end_idx=$LENGTH
          fi
          batch_size=$((end_idx - start_idx))

          echo "Processing batch $((batch_num + 1))/$NUM_BATCHES (components $((start_idx + 1))-$end_idx)"

          # Collect fragments for this batch
          batch_fragments='[]'
          for((i=start_idx; i<end_idx; i++)); do
            fbc_fragment=$(jq -cr --argjson i "$i" '.components[$i].containerImage' "${SNAPSHOT_PATH}")
            batch_fragments=$(jq --arg fragment "$fbc_fragment" '. += [$fragment]' <<< "$batch_fragments")
          done

          echo "Batch $((batch_num + 1)) details:"
          echo "  - fromIndex: $current_from_index"
          echo "  - targetIndex: $batch_target_index"
          echo "  - ocpVersion: $batch_ocp_version"
          echo "  - fragments: $batch_size items"
          if [ $batch_num -gt 0 ]; then
            echo "  - Chaining from previous batch output"
          fi

          # Create InternalRequest for this batch
          echo "Creating InternalRequest for batch $((batch_num + 1)) with $batch_size fragments..."
          internal-request --pipeline "update-fbc-catalog" \
              -p fromIndex="${current_from_index}" \
              -p targetIndex="${batch_target_index}" \
              -p fbcFragments="$(printf '%s' "${batch_fragments}" | jq -c .)" \
              -p iibServiceAccountSecret="${iib_service_account_secret}" \
              -p publishingCredentials="${publishing_credentials}" \
              -p buildTimeoutSeconds="${build_timeout_seconds}" \
              -p buildTags="$(jq -c . <<< "${build_tags}")" \
              -p addArches="$(jq -c . <<< "${add_arches}")" \
              -p hotfix="${hotfix}" \
              -p stagedIndex="${staged_index}" \
              -p taskGitUrl="$(params.taskGitUrl)" \
              -p taskGitRevision="$(params.taskGitRevision)" \
              --service-account "${internal_request_service_account}" \
              -l ${pipelinerun_label}="$(params.pipelineRunUid)" \
              --pipeline-timeout "${pipeline_timeout}" \
              --task-timeout "${task_timeout}" \
              -t "${request_timeout_seconds}" \
              |tee "$(params.dataDir)"/ir-"$(context.taskRun.uid)"-batch-$((batch_num + 1))-output.log

          # Store batch request info in global variables (final batch values will be used later)
          internalRequest=$(awk -F"'" '/created/ { print $2 }' \
            "$(params.dataDir)"/ir-"$(context.taskRun.uid)"-batch-$((batch_num + 1))-output.log)
          echo "Batch $((batch_num + 1)) request created: ${internalRequest}"

          # Fetch batch results (global variables updated each iteration)
          results=$(kubectl get internalrequest "${internalRequest}" -o jsonpath='{.status.results}')
          echo "${results}" > "$(params.dataDir)/ir-$(context.taskRun.uid)-batch-$((batch_num + 1))-result.json"

          # Decompress jsonBuildInfo for processing (global variable updated each iteration)
          decompressed_json_build_info="$(jq -r '.jsonBuildInfo' <<< "${results}" | base64 -d | gunzip)"
          completion_time_raw="$(jq -r '.updated' <<< "${decompressed_json_build_info}")"
          completion_time=$(date +"${timestamp_format}" -d "${completion_time_raw}")
          
          # Process results for each fragment in this batch
          fragment_count=$(jq -r 'length' <<< "$batch_fragments")
          
          for((f=0; f<fragment_count; f++)); do
            fragment=$(jq -r --argjson f "$f" '.[$f]' <<< "$batch_fragments")
            
            echo "Processing result for fragment: $fragment (batch $((batch_num + 1)))"
            
            # Build individual component result
            build_results=$(jq \
              --arg fragment "$fragment" \
              --arg target_index "$batch_target_index" \
              --arg ocp_version "$batch_ocp_version" \
              --arg completion_time "$completion_time" \
              --argjson decompressed_json "${decompressed_json_build_info}" \
            '{
              "fbc_fragment": $fragment,
              "target_index": $target_index,
              "ocp_version": $ocp_version,
              "image_digests": (.indexImageDigests | split(" ") | del(.[] | select(. == ""))),
              "index_image": $decompressed_json.index_image,
              "index_image_resolved": $decompressed_json.index_image_resolved,
              "completion_time": $completion_time,
              "iibLog": .iibLog
            }' <<< "${results}")

            # Add to results file
            export build_results
            yq -i '.components += [ env(build_results) ]' "$RESULTS_FILE"
          done
          
          # Update fromIndex for next batch (if there is one) to chain index images
          if [ $batch_num -lt $((NUM_BATCHES - 1)) ]; then
            # Extract the output index image from this batch to use as input for next batch
            next_from_index=$(jq -r '.index_image' <<< "$decompressed_json_build_info")
            if [ -n "$next_from_index" ] && [ "$next_from_index" != "null" ]; then
              current_from_index="$next_from_index"
              echo "Next batch will use fromIndex: $current_from_index"
            else
              echo "Warning: Could not extract index_image from batch $((batch_num + 1)) results, " \
                   "using original fromIndex"
            fi
          fi
        done

        # Extract flags from batch results (same for all fragments in batch)
        mustSignIndexImage=$(jq -r '.genericResult | fromjson' <<< "${results}" \
          | jq -r '.sign_index_image' |tr -d "\n")
        mustPublishIndexImage=$(jq -r '.genericResult | fromjson' <<< "${results}" \
          | jq -r '.publish_index_image' |tr -d "\n")
        fbc_opt_in=$(jq -r '.genericResult | fromjson' <<< "${results}" | jq -cr '.fbc_opt_in')

        # Store the results in Tekton's results files
        echo -en "${mustPublishIndexImage}" | tee "$(results.mustPublishIndexImage.path)"
        echo -en "${mustSignIndexImage}" | tee "$(results.mustSignIndexImage.path)"
        echo -en "${fbc_opt_in}" | tee "$(results.isFbcOptIn.path)"

        # Store deprecated results for compatibility
        conditions=$(kubectl get internalrequest "${internalRequest}" \
          -o jsonpath='{.status.conditions[?(@.type=="Succeeded")]}')
        jq '.reason // "Unset"'  <<< "${conditions}" | tee "$(results.requestReason.path)"
        jq '.message // "Unset"' <<< "${conditions}" | tee "$(results.requestMessage.path)"
        jq -r '.indexImageDigests' <<< "${results}" |  tee "$(results.indexImageDigests.path)"

        # Output batch logs for debugging
        jq -r '.iibLog' <<< "${results}"
        RC="$(jq -r '.exitCode' <<< "${results}")"

        # Summarize batch results
        if [ "$mustPublishIndexImage" = "true" ]; then
          echo "Index image will be published."
        elif [ "$fbc_opt_in" = "false" ]; then
          echo "Index image will not be published because fbc_opt_in is set to false in Pyxis."
          echo "If this is the first time you are releasing, make sure you request fbc_opt_in in Pyxis."
        elif [ "${staged_index}" = "true" ]; then
          echo "Index image will not be published because this is a staging release."
        else
          echo "Index image will not be published for an unspecified reason."
        fi

        if [ "$RC" -ne 0 ]; then
          echo "Batch processing failed, check the batch logs above to understand the reason"
          exit "$RC"
        fi
        
        echo "Batch processing completed successfully with $LENGTH components"
        echo "Results file:"
        cat "$RESULTS_FILE"
    - name: create-trusted-artifact
      computeResources:
        limits:
          memory: 128Mi
        requests:
          memory: 128Mi
          cpu: 250m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/create-trusted-artifact/create-trusted-artifact.yaml
      params:
        - name: ociStorage
          value: $(params.ociStorage)
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(results.sourceDataArtifact.path)
