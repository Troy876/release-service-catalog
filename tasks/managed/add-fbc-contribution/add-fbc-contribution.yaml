---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: add-fbc-contribution
  annotations:
    tekton.dev/pipelines.minVersion: "0.12.1"
    tekton.dev/tags: release
spec:
  description: |-
    Task to create an internalrequest to add fbc contributions to index images
  params:
    - name: snapshotPath
      description: Path to the JSON string of the mapped Snapshot spec in the data workspace
      type: string
    - name: dataPath
      description: Path to the JSON string of the merged data to use in the data workspace
      type: string
    - name: fromIndex
      type: string
      description: fromIndex value updated by update-ocp-tag task
    - name: pipelineRunUid
      type: string
      description: The uid of the current pipelineRun. Used as a label value when creating internal requests
    - name: ocpVersion
      type: string
      description: The OCP version for all components in this release
    - name: resultsDirPath
      type: string
      description: Path to the results directory in the data workspace
    - name: ociStorage
      description: The OCI repository where the Trusted Artifacts are stored
      type: string
      default: "empty"
    - name: ociArtifactExpiresAfter
      description: Expiration date for the trusted artifacts created in the
        OCI repository. An empty string means the artifacts do not expire
      type: string
      default: "1d"
    - name: trustedArtifactsDebug
      description: Flag to enable debug logging in trusted artifacts. Set to a non-empty string to enable
      type: string
      default: ""
    - name: orasOptions
      description: oras options to pass to Trusted Artifacts calls
      type: string
      default: ""
    - name: sourceDataArtifact
      type: string
      description: Location of trusted artifacts to be used to populate data directory
      default: ""
    - name: dataDir
      description: The location where data will be stored
      type: string
      default: /var/workdir/release
    - name: taskGitUrl
      type: string
      description: The url to the git repo where the release-service-catalog tasks and stepactions to be used are stored
    - name: taskGitRevision
      type: string
      description: The revision in the taskGitUrl repo to be used
    - name: maxBatchSize
      type: string
      description: Maximum number of FBC fragments to process in a single batch
      default: "5"
    - name: mustPublishIndexImage
      type: string
      description: Whether the index image should be published (from prepare-fbc-parameters)
    - name: mustOverwriteFromIndexImage
      type: string
      description: Whether to overwrite the from index image (from prepare-fbc-parameters)
    - name: iibServiceAccountSecret
      type: string
      description: IIB service account secret name (from prepare-fbc-parameters)
    - name: resolvedTargetIndex
      type: string
      description: Resolved target index with sanitized tag (from prepare-fbc-parameters)
    - name: maxRetries
      description: Maximum number of retry attempts for failed internal requests
      type: string
      default: "3"
    - name: batchRetryDelaySeconds
      description: Delay between batch retry attempts in seconds
      type: string
      default: "60"
  results:
    - name: buildTimestamp
      description: Build timestamp used in the tag
    - name: requestResultsFile
      description: Internal Request results file
    - name: internalRequestResultsFile
      description: Additional Internal Request results file
    - name: requestMessage
      description: Internal Request message
    - name: requestReason
      description: Internal Request reason
    - name: indexImageDigests
      description: list of manifest digests for each arch from manifest list in index image
    - name: sourceDataArtifact
      type: string
      description: Produced trusted data artifact
  volumes:
    - name: workdir
      emptyDir: {}
  stepTemplate:
    volumeMounts:
      - mountPath: /var/workdir
        name: workdir
    env:
      - name: IMAGE_EXPIRES_AFTER
        value: $(params.ociArtifactExpiresAfter)
      - name: "ORAS_OPTIONS"
        value: "$(params.orasOptions)"
      - name: "DEBUG"
        value: "$(params.trustedArtifactsDebug)"
  steps:
    - name: use-trusted-artifact
      computeResources:
        limits:
          memory: 64Mi
        requests:
          memory: 64Mi
          cpu: 30m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/use-trusted-artifact/use-trusted-artifact.yaml
      params:
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(params.sourceDataArtifact)
    - name: add-contribution
      image: quay.io/konflux-ci/release-service-utils:9c82b0f13e40e76150835b628c86ce05ae95a366
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi  # was exiting with code 137 when set to 256Mi
          cpu: 200m
      script: |
        #!/usr/bin/env bash
        #
        set -eo pipefail

        SNAPSHOT_PATH="$(params.dataDir)/$(params.snapshotPath)"
        DATA_FILE="$(params.dataDir)/$(params.dataPath)"
        if [ ! -f "${DATA_FILE}" ] ; then
            echo "No valid data file was provided."
            exit 1
        fi

        # adding a new result as modifying the current one used breaks e2e for single component.
        # to be handled in RELEASE-1640.
        RESULTS_FILE="$(params.resultsDirPath)/internal-requests-results.json"
        echo -n "$RESULTS_FILE" > "$(results.internalRequestResultsFile.path)"
        RESULTS_FILE="$(params.dataDir)/$(params.resultsDirPath)/internal-requests-results.json"

        echo -n "$(params.dataDir)/$(params.pipelineRunUid)/ir-$(context.taskRun.uid)-result.json" \
          > "$(results.requestResultsFile.path)"

        default_build_timeout_seconds="3600"
        default_request_timeout_seconds="3600"

        build_timeout_seconds=$(jq -r --arg build_timeout_seconds "${default_build_timeout_seconds}" \
            '.fbc.buildTimeoutSeconds // $build_timeout_seconds' "${DATA_FILE}")
        request_timeout_seconds=$(jq -r --arg request_timeout_seconds "${default_request_timeout_seconds}" \
            '.fbc.requestTimeoutSeconds // $request_timeout_seconds' "${DATA_FILE}")
        internal_request_service_account=$(jq -r '.fbc.internalRequestServiceAccount // "release-service-account"' \
            "${DATA_FILE}")
        publishing_credentials=$(jq -r '.fbc.publishingCredentials // "catalog-publishing-secret"' "$DATA_FILE")

        # Use pre-determined values from prepare-fbc-parameters
        iib_service_account_secret="$(params.iibServiceAccountSecret)"
        mustPublishIndexImage="$(params.mustPublishIndexImage)"
        mustOverwriteFromIndexImage="$(params.mustOverwriteFromIndexImage)"

        timestamp_format=$(jq -r '.fbc.timestampFormat // "%s"' "${DATA_FILE}")
        timestamp=$(date "+${timestamp_format}")

        # Use resolved target index from prepare-fbc-parameters
        target_index="$(params.resolvedTargetIndex)"

        echo "INFO: Using pre-determined values from prepare-fbc-parameters:"
        echo "  - mustPublishIndexImage: ${mustPublishIndexImage}"
        echo "  - mustOverwriteFromIndexImage: ${mustOverwriteFromIndexImage}"
        echo "  - iibServiceAccountSecret: ${iib_service_account_secret}"
        echo "  - resolvedTargetIndex: ${target_index}"

        # to keep compatibility with current single component mode
        echo -n "$timestamp" > "$(results.buildTimestamp.path)"
        jq -n --arg target_index "$target_index" \
          '{"index_image": {"target_index": $target_index}, "components": []}' \
          | tee "$RESULTS_FILE"

        # Setup consistent labeling for FBC internal requests
        TASK_LABEL="internal-services.appstudio.openshift.io/group-id"
        TASK_ID=$(context.taskRun.uid)
        PIPELINERUN_LABEL="internal-services.appstudio.openshift.io/pipelinerun-uid"

        # Create batches with size limits
        LENGTH=$(jq -r '.components | length' "${SNAPSHOT_PATH}")
        MAX_BATCH_SIZE="$(params.maxBatchSize)"
        echo "Processing $LENGTH components with maximum batch size of $MAX_BATCH_SIZE..."

        # Read global buildTags and addArches from data file
        build_tags=$(jq '.fbc.buildTags // []' "${DATA_FILE}")
        add_arches=$(jq '.fbc.addArches // []' "${DATA_FILE}")

        # Extract tag from target index for PLR identification
        target_tag=$(printf '%s' "$(params.resolvedTargetIndex)" | rev | cut -d: -f1 | rev)

        # If target has no tag, use tag from fromIndex
        if [ "${target_tag}" = "$(params.resolvedTargetIndex)" ]; then
            # No tag in target index, extract from fromIndex
            target_tag=$(printf '%s' "$(params.fromIndex)" | rev | cut -d: -f1 | rev)
        fi

        # Add PLR identifier to build tags (appends to existing tags)
        build_tags=$(echo "${build_tags}" | jq --arg tag "${target_tag}" '. + [$tag]')
        echo "Added PLR identifier to build_tags: ${target_tag}"
        echo "Final build_tags: $(echo "${build_tags}" | jq -r '.[]' | tr '\n' ' ')"

        # Use the same target_index for batch processing (already resolved above)
        resolved_target_index="$target_index"

        # Calculate number of batches needed
        NUM_BATCHES=$(( (LENGTH + MAX_BATCH_SIZE - 1) / MAX_BATCH_SIZE ))
        echo "Creating $NUM_BATCHES batch(es) for $LENGTH components"

        # Common values for all batches
        batch_from_index="$(params.fromIndex)"
        batch_target_index="$resolved_target_index"
        batch_ocp_version="$(params.ocpVersion)"
        
        # Calculate timeout for batches
        finally_task_timeout=300
        pipeline_timeout=$(date -u "+%Hh%Mm%Ss" -d @$(( request_timeout_seconds + finally_task_timeout )))
        task_timeout=$(date -u "+%Hh%Mm%Ss" -d @$(( request_timeout_seconds )))

        # Initialize tracking for batch chaining
        current_from_index="$batch_from_index"
        latest_iib_index_image=""

        # Helper function to determine fromIndex strategy based on release type
        get_current_from_index() {
            if [ "${mustOverwriteFromIndexImage}" = "true" ]; then
                # IIB overwrites fromIndex - keep using original consistently
                printf '%s' "$(params.fromIndex)"
            else
                # IIB creates new index - we MUST track what was actually created
                if [ ${#successful_batches[@]} -eq 0 ]; then
                    # No successful batches yet - start from original fromIndex
                    printf '%s' "$(params.fromIndex)"
                else
                    # We have successful batches - MUST use latest IIB output
                    if [ -z "${latest_iib_index_image}" ]; then
                        echo "FATAL ERROR: We have successful batches but latest_iib_index_image is empty!"
                        echo "This would cause data loss. Successful batches: ${successful_batches[*]}"
                        exit 1
                    fi
                    echo "${latest_iib_index_image}"
                fi
            fi
        }

        # Helper function to get batch fragments
        get_batch_fragments() {
            local batch_num="$1"
            local start_idx
            start_idx=$((batch_num * MAX_BATCH_SIZE))
            local end_idx
            end_idx=$(((batch_num + 1) * MAX_BATCH_SIZE))
            if [ "$end_idx" -gt "$LENGTH" ]; then
                end_idx=$LENGTH
            fi

            local batch_fragments
            batch_fragments='[]'
            for((i=start_idx; i<end_idx; i++)); do
                local fbc_fragment
                fbc_fragment=$(jq -cr --argjson i "$i" '.components[$i].containerImage' "${SNAPSHOT_PATH}")
                batch_fragments=$(jq --arg fragment "$fbc_fragment" '. += [$fragment]' <<< "$batch_fragments")
            done
            echo "$batch_fragments"
        }

        # Helper function to execute batch with validation
        execute_batch_with_validation() {
            local batch_num="$1"
            local from_index="$2"
            local target_index="$3"

            echo "Executing batch $((batch_num + 1)): fromIndex=${from_index}"

            local batch_fragments
            batch_fragments=$(get_batch_fragments "$batch_num")
            # Create InternalRequest for this batch
            internal-request --pipeline "update-fbc-catalog" \
                -p fromIndex="${from_index}" \
                -p fbcFragments="$(printf '%s' "${batch_fragments}" | jq -c .)" \
                -p iibServiceAccountSecret="${iib_service_account_secret}" \
                -p publishingCredentials="${publishing_credentials}" \
                -p buildTimeoutSeconds="${build_timeout_seconds}" \
                -p buildTags="$(jq -c . <<< "${build_tags}")" \
                -p addArches="$(jq -c . <<< "${add_arches}")" \
                -p mustPublishIndexImage="${mustPublishIndexImage}" \
                -p mustOverwriteFromIndexImage="${mustOverwriteFromIndexImage}" \
                -p taskGitUrl="$(params.taskGitUrl)" \
                -p taskGitRevision="$(params.taskGitRevision)" \
                --service-account "${internal_request_service_account}" \
                -l ${TASK_LABEL}="${TASK_ID}" \
                -l ${PIPELINERUN_LABEL}="$(params.pipelineRunUid)" \
                --pipeline-timeout "${pipeline_timeout}" \
                --task-timeout "${task_timeout}" \
                -t "${request_timeout_seconds}" \
                | tee "$(params.dataDir)/ir-$(context.taskRun.uid)-batch-$((batch_num + 1))-output.log"

            # Extract request name
            local internalRequest
            internalRequest=$(awk -F"'" '/created/ { print $2 }' \
                "$(params.dataDir)/ir-$(context.taskRun.uid)-batch-$((batch_num + 1))-output.log")

            # Validate internal request succeeded
            local request_status
            request_status=$(kubectl get internalrequest "${internalRequest}" \
                -o jsonpath='{.status.conditions[?(@.type=="Succeeded")].status}')

            if [ "${request_status}" != "True" ]; then
                echo "ERROR: Batch $((batch_num + 1)) internal request failed"
                local request_reason
                request_reason=$(kubectl get internalrequest "${internalRequest}" \
                    -o jsonpath='{.status.conditions[?(@.type=="Succeeded")].reason}')
                local request_message
                request_message=$(kubectl get internalrequest "${internalRequest}" \
                    -o jsonpath='{.status.conditions[?(@.type=="Succeeded")].message}')
                echo "Reason: ${request_reason}"
                echo "Message: ${request_message}"
                return 1
            fi

            # Extract and validate results
            local results
            results=$(kubectl get internalrequest "${internalRequest}" -o jsonpath='{.status.results}')

            if [ -z "${results}" ] || [ "${results}" = "{}" ]; then
                echo "ERROR: Batch $((batch_num + 1)) succeeded but returned empty results"
                return 1
            fi

            # Store results for this batch
            echo "${results}" > "$(params.dataDir)/ir-$(context.taskRun.uid)-batch-$((batch_num + 1))-result.json"

            # Process results for each fragment in this batch
            local decompressed_json_build_info
            decompressed_json_build_info="$(jq -r '.jsonBuildInfo' <<< "${results}" | base64 -d | gunzip)"
            local completion_time_raw
            completion_time_raw="$(jq -r '.updated' <<< "${decompressed_json_build_info}")"
            local completion_time
            completion_time=$(date +"${timestamp_format}" -d "${completion_time_raw}")
            local fragment_count
            fragment_count=$(jq -r 'length' <<< "$batch_fragments")

            for((f=0; f<fragment_count; f++)); do
                local fragment
                fragment=$(jq -r --argjson f "$f" '.[$f]' <<< "$batch_fragments")

                echo "Processing result for fragment: $fragment (batch $((batch_num + 1)))"

                # Build individual component result
                local build_results
                build_results=$(jq \
                  --arg fragment "$fragment" \
                  --arg target_index "$target_index" \
                  --arg ocp_version "$batch_ocp_version" \
                  --arg completion_time "$completion_time" \
                  --argjson decompressed_json "${decompressed_json_build_info}" \
                '{
                  "fbc_fragment": $fragment,
                  "target_index": $target_index,
                  "ocp_version": $ocp_version,
                  "image_digests": (.indexImageDigests | split(" ") | del(.[] | select(. == ""))),
                  "index_image": $decompressed_json.index_image,
                  "index_image_resolved": $decompressed_json.index_image_resolved,
                  "completion_time": $completion_time,
                  "iibLog": .iibLog
                }' <<< "${results}")

                # Add to results file
                export build_results
                yq -i '.components += [ env(build_results) ]' "$RESULTS_FILE"
            done

            echo "Batch $((batch_num + 1)) completed successfully"
            return 0
        }

        # Execute batches with failure tracking and retry logic
        failed_batches=()
        successful_batches=()

        for((batch_num=0; batch_num<"$NUM_BATCHES"; batch_num++)); do
            echo "Processing batch $((batch_num + 1))/${NUM_BATCHES}..."

            current_from_index=$(get_current_from_index)

            if execute_batch_with_validation "$batch_num" "$current_from_index" "$batch_target_index"; then
                successful_batches+=("$batch_num")
                echo "Batch $((batch_num + 1)) succeeded"

                # Extract latest IIB index image for next batch (only when not overwriting)
                if [ "${mustOverwriteFromIndexImage}" = "false" ]; then
                    result_file="$(params.dataDir)/ir-$(context.taskRun.uid)-batch-$((batch_num + 1))-result.json"
                    batch_results=$(cat "${result_file}")
                    latest_iib_index_image=$(jq -r '.jsonBuildInfo' <<< "${batch_results}" | \
                        base64 -d | gunzip | jq -r '.index_image')
                    echo "Updated fromIndex for next batch: ${latest_iib_index_image}"
                fi
            else
                failed_batches+=("$batch_num")
                echo "Batch $((batch_num + 1)) failed, will retry later"
                # Continue with next batch - no dependency blocking
            fi
        done

        # Retry failed batches (order doesn't matter)
        max_retries=$(params.maxRetries)
        batch_retry_delay=$(params.batchRetryDelaySeconds)

        for retry_attempt in $(seq 1 "$max_retries"); do
            if [ ${#failed_batches[@]} -eq 0 ]; then
                echo "All batches completed successfully"
                break
            fi

            echo "Retry attempt ${retry_attempt}: ${#failed_batches[@]} batches to retry"
            still_failed=()

            for batch_num in "${failed_batches[@]}"; do
                current_from_index=$(get_current_from_index)

                if execute_batch_with_validation "$batch_num" "$current_from_index" "$batch_target_index"; then
                    echo "Batch $((batch_num + 1)) succeeded on retry attempt ${retry_attempt}"

                    # Add to successful batches and extract latest IIB index image (only when not overwriting)
                    successful_batches+=("$batch_num")
                    if [ "${mustOverwriteFromIndexImage}" = "false" ]; then
                        result_file="$(params.dataDir)/ir-$(context.taskRun.uid)-batch-$((batch_num + 1))-result.json"
                    batch_results=$(cat "${result_file}")
                        latest_iib_index_image=$(jq -r '.jsonBuildInfo' <<< "${batch_results}" | \
                            base64 -d | gunzip | jq -r '.index_image')
                        echo "Updated fromIndex for next batch: ${latest_iib_index_image}"
                    fi
                else
                    still_failed+=("$batch_num")
                    echo "Batch $((batch_num + 1)) failed retry attempt ${retry_attempt}"
                fi
            done

            failed_batches=("${still_failed[@]}")

            if [ ${#failed_batches[@]} -gt 0 ] && [ "${retry_attempt}" -lt "${max_retries}" ]; then
                echo "Waiting ${batch_retry_delay} seconds before next retry attempt..."
                sleep "${batch_retry_delay}"
            fi
        done

        # Final validation - CRITICAL for data safety
        if [ ${#failed_batches[@]} -gt 0 ]; then
            echo "ERROR: ${#failed_batches[@]} batches failed after all retries:"
            for batch_num in "${failed_batches[@]}"; do
                echo "  - Batch $((batch_num + 1))"
            done
            echo "Cannot proceed with partial index - this would result in incomplete fragment coverage"
            exit 1
        fi

        echo "SUCCESS: All ${NUM_BATCHES} batches completed successfully"

        # Get the final batch result for legacy compatibility (use last successful batch)
        last_batch_num=$((NUM_BATCHES - 1))
        results=$(cat "$(params.dataDir)/ir-$(context.taskRun.uid)-batch-$((last_batch_num + 1))-result.json")
        internalRequest=$(awk -F"'" '/created/ { print $2 }' \
            "$(params.dataDir)/ir-$(context.taskRun.uid)-batch-$((last_batch_num + 1))-output.log")

        echo "Final publishing and compatibility values:"
        echo "  - mustPublishIndexImage: ${mustPublishIndexImage}"

        # Store the results in Tekton's results files

        # Store deprecated results for compatibility
        conditions=$(kubectl get internalrequest "${internalRequest}" \
          -o jsonpath='{.status.conditions[?(@.type=="Succeeded")]}')
        jq '.reason // "Unset"'  <<< "${conditions}" | tee "$(results.requestReason.path)"
        jq '.message // "Unset"' <<< "${conditions}" | tee "$(results.requestMessage.path)"
        jq -r '.indexImageDigests' <<< "${results}" |  tee "$(results.indexImageDigests.path)"

        # Output batch logs for debugging
        jq -r '.iibLog' <<< "${results}"
        RC="$(jq -r '.exitCode' <<< "${results}")"

        # Summarize batch results
        if [ "$mustPublishIndexImage" = "true" ]; then
          echo "Index image will be published."
        else
          echo "Index image will not be published (decision made by prepare-fbc-parameters)."
        fi

        if [ "$RC" -ne 0 ]; then
          echo "Batch processing failed, check the batch logs above to understand the reason"
          exit "$RC"
        fi
        
        echo "Batch processing completed successfully with $LENGTH components"
        echo "Results file:"
        cat "$RESULTS_FILE"
    - name: create-trusted-artifact
      computeResources:
        limits:
          memory: 128Mi
        requests:
          memory: 128Mi
          cpu: 250m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/create-trusted-artifact/create-trusted-artifact.yaml
      params:
        - name: ociStorage
          value: $(params.ociStorage)
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(results.sourceDataArtifact.path)
