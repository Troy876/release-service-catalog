---
apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: upload-sbom-to-atlas
  annotations:
    tekton.dev/pipelines.minVersion: "0.12.1"
    tekton.dev/tags: release
spec:
  description: |-
    This Tekton task gathers SBOM data from a directory specified by the parameters
    and uploads them to Atlas. Supports both CycloneDX and SPDX format. If the push
    to Atlas fails, the SBOM is pushed to an S3 bucket. The push to Atlas is then
    retried asynchronously from the bucket by another service. (Bombino)

    The provided directory is searched for SBOMs recursively and all found SBOMs
    are uploaded as-is to Atlas.

    Optionally, if the dataPath, snapshotSpec and releaseId parameters are
    provided, pushes SBOM regeneration data to the S3 bucket.
  params:
    - name: dataPath
      description: Relative path to the JSON data file in the workspace
      type: string
      default: ""
    - name: snapshotSpec
      type: string
      description: Path to the mapped snapshot spec
      default: ""
    - name: releaseId
      type: string
      description: Release ID to name SBOM regeneration data with
      default: ""
    - name: sbomDir
      description: >-
        Directory containing SBOM files. The task will search for JSON
        SBOMs recursively in this directory and upload them all to Atlas.
        The path is relative to the 'data' workspace
      type: string
    - name: httpRetries
      default: "3"
      description: Maximum number of retries for transient HTTP(S) errors
      type: string
    - name: atlasSecretName
      default: atlas-prod-sso-secret
      description: Name of the Secret containing SSO auth credentials for Atlas
      type: string
    - name: atlasApiUrl
      default: "https://atlas.release.devshift.net"
      description: URL of the Atlas API host
      type: string
    - name: ssoTokenUrl
      default: "https://auth.redhat.com/auth/realms/EmployeeIDP/protocol/openid-connect/token"
      description: URL of the SSO token issuer
      type: string
    - name: ociStorage
      description: The OCI repository where the Trusted Artifacts are stored
      type: string
      default: "empty"
    - name: ociArtifactExpiresAfter
      description: Expiration date for the trusted artifacts created in the
        OCI repository. An empty string means the artifacts do not expire
      type: string
      default: "1d"
    - name: trustedArtifactsDebug
      description: Flag to enable debug logging in trusted artifacts. Set to a non-empty string to enable
      type: string
      default: ""
    - name: orasOptions
      description: oras options to pass to Trusted Artifacts calls
      type: string
      default: ""
    - name: sourceDataArtifact
      type: string
      description: Location of trusted artifacts to be used to populate data directory
      default: ""
    - name: subdirectory
      # subdirectory is only needed for testing purposes
      description: Subdirectory inside the workspace to be used
      type: string
      default: ""
    - name: dataDir
      description: The location where data will be stored
      type: string
      default: $(workspaces.data.path)
    - name: taskGitUrl
      type: string
      description: The url to the git repo where the release-service-catalog tasks and stepactions to be used are stored
    - name: taskGitRevision
      type: string
      description: The revision in the taskGitUrl repo to be used
    - name: retryAWSSecretName
      description: Name of the Secret containing AWS credentials for retry mechanism
    - name: retryS3Bucket
      description: Name of the S3 bucket for the retry mechanism
    - name: concurrentLimit
      type: string
      description: The maximum number of SBOMs to be uploaded at once
      default: 10
  workspaces:
    - name: data
  results:
    - description: Produced trusted data artifact
      name: sourceDataArtifact
      type: string
  volumes:
    - name: atlas-secret
      secret:
        secretName: $(params.atlasSecretName)
    - name: aws-secret
      secret:
        secretName: $(params.retryAWSSecretName)
    - name: workdir
      emptyDir: {}
  stepTemplate:
    volumeMounts:
      - mountPath: /var/workdir
        name: workdir
    env:
      - name: IMAGE_EXPIRES_AFTER
        value: $(params.ociArtifactExpiresAfter)
      - name: "ORAS_OPTIONS"
        value: "$(params.orasOptions)"
      - name: "DEBUG"
        value: "$(params.trustedArtifactsDebug)"
  steps:
    - name: skip-trusted-artifact-operations
      computeResources:
        limits:
          memory: 32Mi
        requests:
          memory: 32Mi
          cpu: 20m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/skip-trusted-artifact-operations/skip-trusted-artifact-operations.yaml
      params:
        - name: ociStorage
          value: $(params.ociStorage)
        - name: workDir
          value: $(params.dataDir)
    - name: use-trusted-artifact
      computeResources:
        limits:
          memory: 64Mi
        requests:
          memory: 64Mi
          cpu: 30m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/use-trusted-artifact/use-trusted-artifact.yaml
      params:
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(params.sourceDataArtifact)
    - name: upload-sboms
      image: quay.io/konflux-ci/release-service-utils:e633d51cd41d73e4b3310face21bb980af7a662f
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi
          cpu: 10m
      volumeMounts:
        - name: atlas-secret
          mountPath: /secrets/
      script: |
        #!/usr/bin/env bash
        set -o errexit -o pipefail -o nounset

        sbomsDir="$(params.dataDir)/$(params.sbomDir)"
        failedSbomDir="$(params.dataDir)/$(params.subdirectory)/failed-sboms"
        # make sure the directory wasn't created in a previous run of this task
        rm -rf "$failedSbomDir"
        mkdir -p "$failedSbomDir"

        shopt -s nullglob
        sboms_to_upload=("$sbomsDir"/*)

        if [[ "${#sboms_to_upload[@]}" -eq 0 ]]; then
          echo "No SBOMs to upload"
          exit 0
        fi

        httpRetries=$(params.httpRetries)
        curl_opts=(--silent --show-error --fail-with-body --retry "$httpRetries")

        sso_account="$(cat /secrets/sso_account)"
        sso_token="$(cat /secrets/sso_token)"

        # Function to upload a single SBOM to Atlas
        upload_sbom_to_atlas() {
            local sbom_path="$1"
            local access_token="$2"
            local atlas_api_url="$3"
            local retry_max_time="$4"
            local failed_sbom_dir="$5"
            local sbom_filename
            sbom_filename=$(basename "$sbom_path")
            
            echo "Uploading SBOM $sbom_path to $atlas_api_url"
            
            handle_atlas_failure() {
                >&2 echo "WARNING: SBOM upload of $1 to Atlas has failed! Upload will be retried later."
                cp "$1" "${failed_sbom_dir}"
            }

            if ! curl -X POST "${curl_opts[@]}" \
                --retry-max-time "$retry_max_time" \
                -H "authorization: Bearer $access_token" \
                -H "transfer-encoding: chunked" \
                -H "content-type: application/json" \
                --data "@$sbom_path" \
                "$atlas_api_url/api/v2/sbom"; then
                handle_atlas_failure "$sbom_path"
            fi

            # In the stage environment (and e2e tests), retry the push of all
            # SBOMs to test functionality of the retry mechanism.
            if [[ "$(params.atlasSecretName)" = "atlas-staging-sso-secret" ]]; then
                cp "$sbom_path" "${failed_sbom_dir}"
            fi
        }

        # Get SSO token once for all uploads
        ssoTokenUrl=$(params.ssoTokenUrl)
        echo "Getting SSO token from $ssoTokenUrl"

        token_response="$(
          curl -X POST "${curl_opts[@]}" \
            -d "grant_type=client_credentials" \
            -d "client_id=$sso_account" \
            -d "client_secret=$sso_token" \
            "$ssoTokenUrl"
        )"

        # https://www.rfc-editor.org/rfc/rfc6749.html#section-5.1
        access_token="$(jq -r .access_token <<< "$token_response")"
        expires_in="$(jq -r ".expires_in // empty" <<< "$token_response")"

        retry_max_time=0  # no limit
        if [[ -n "$expires_in" ]]; then
          retry_max_time="$expires_in"
        fi

        atlasApiUrl=$(params.atlasApiUrl)

        # Process uploads in parallel batches
        N=$(params.concurrentLimit)  # The maximum number of SBOMs to be uploaded at once
        declare -a jobs=()
        declare -a job_outputs=()
        total=${#sboms_to_upload[@]}
        count=0
        success=true
        echo "Starting SBOM upload for $total files in total. " \
          "Up to $N files will be uploaded at once..."

        # Create temporary directory for output files
        temp_output_dir=$(mktemp -d)
        trap 'rm -rf "$temp_output_dir"' EXIT

        for sbom_path in "${sboms_to_upload[@]}"; do
          sbom_filename=$(basename "$sbom_path")
          output_file="${temp_output_dir}/${sbom_filename}.upload.out"
          echo "Starting upload for SBOM: $sbom_filename"
          
          upload_sbom_to_atlas "$sbom_path" "$access_token" "$atlasApiUrl" \
            "$retry_max_time" "$failedSbomDir" > "$output_file" 2>&1 &
          
          jobs+=($!)  # Save the background process ID
          job_outputs+=("$output_file")
          ((++count))

          if [ $((count%N)) -eq 0 ] || [ $((count)) -eq "$total" ]; then
            echo "Waiting for the current batch of background processes to finish"
            for job_id in "${!jobs[@]}"; do
              if ! wait "${jobs[job_id]}"; then
                echo "Error: upload of sbom failed for one of the files"
                success=false
              fi
            done

            echo
            echo "Printing outputs for current upload runs"
            for output_file in "${job_outputs[@]}"; do
              sbom_name=$(basename "$output_file" .upload.out)
              echo "=== $sbom_name ==="
              if [ -f "$output_file" ]; then
                cat "$output_file"
              else
                echo "Output file not found: $output_file"
              fi
              echo
            done

            if [ $success != "true" ]; then
              echo "ERROR: At least one upload in the last batch failed"
              exit 1
            fi

            # Reset job arrays for the next batch
            jobs=()
            job_outputs=()
          fi
        done

    - name: push-to-s3
      image: quay.io/konflux-ci/release-service-utils:e85ceb962ee6f4d0672b4aa4e9946621ab302f20
      computeResources:
        limits:
          memory: 512Mi
        requests:
          memory: 512Mi
          cpu: 10m
      volumeMounts:
        - name: aws-secret
          mountPath: /secrets
      script: |
        #!/usr/bin/env bash
        set -o pipefail -o nounset

        aws_access_key_id="$(cat /secrets/atlas-aws-access-key-id)"
        aws_secret_access_key="$(cat /secrets/atlas-aws-secret-access-key)"

        sbomDir="$(params.dataDir)/$(params.subdirectory)/failed-sboms"
        echo "$(find "$sbomDir" -type f | wc -l) SBOMs to upload to S3."

        region="us-east-1"
        bucket="$(params.retryS3Bucket)"

        # Function to upload a single SBOM to S3
        upload_sbom_to_s3() {
            local sbom="$1"
            local bucket="$2"
            local region="$3"
            local aws_access_key_id="$4"
            local aws_secret_access_key="$5"
            
            # we don't want the full path in S3
            local s3_filename
            s3_filename=$(basename "$sbom")
            local resource="/${bucket}/${s3_filename}"
            local content_type="application/json"
            local date_value
            date_value=$(date -R)
            local to_sign="PUT\n\n${content_type}\n${date_value}\n${resource}"
            local signature
            signature=$(echo -en "${to_sign}" | openssl sha1 -hmac "${aws_secret_access_key}" -binary | base64)

            echo "Pushing $sbom to S3."
            if ! curl -X PUT --upload-file "${sbom}" \
                --silent --show-error --fail-with-body --retry 10 \
                --retry-all-errors \
                -H "Host: ${bucket}.s3.${region}.amazonaws.com" \
                -H "Date: ${date_value}" \
                -H "Content-Type: ${content_type}" \
                -H "Authorization: AWS ${aws_access_key_id}:${signature}" \
                "https://${bucket}.s3.${region}.amazonaws.com/${s3_filename}"; then
              >&2 echo "ERROR: Failed to push SBOM to S3 bucket."
              return 1
            fi
        }

        shopt -s nullglob
        sboms_to_s3=("$sbomDir"/*)
        
        if [[ "${#sboms_to_s3[@]}" -eq 0 ]]; then
          echo "No SBOMs to upload to S3"
          exit 0
        fi

        # Process S3 uploads in parallel batches
        N=$(params.concurrentLimit)  # The maximum number of SBOMs to be uploaded at once
        declare -a jobs=()
        declare -a job_outputs=()
        total=${#sboms_to_s3[@]}
        count=0
        success=true
        echo "Starting S3 upload for $total files in total. " \
          "Up to $N files will be uploaded at once..."

        # Create temporary directory for output files
        temp_output_dir=$(mktemp -d)
        trap 'rm -rf "$temp_output_dir"' EXIT

        for sbom in "${sboms_to_s3[@]}"; do
          sbom_filename=$(basename "$sbom")
          output_file="${temp_output_dir}/${sbom_filename}.s3.out"
          echo "Starting S3 upload for SBOM: $sbom_filename"
          
          upload_sbom_to_s3 "$sbom" "$bucket" "$region" "$aws_access_key_id" \
            "$aws_secret_access_key" > "$output_file" 2>&1 &
          
          jobs+=($!)  # Save the background process ID
          job_outputs+=("$output_file")
          ((++count))

          if [ $((count%N)) -eq 0 ] || [ $((count)) -eq "$total" ]; then
            echo "Waiting for the current batch of background processes to finish"
            for job_id in "${!jobs[@]}"; do
              if ! wait "${jobs[job_id]}"; then
                echo "Error: upload of sbom to S3 failed for one of the files"
                success=false
              fi
            done

            echo
            echo "Printing outputs for current S3 upload runs"
            for output_file in "${job_outputs[@]}"; do
              sbom_name=$(basename "$output_file" .s3.out)
              echo "=== $sbom_name ==="
              if [ -f "$output_file" ]; then
                cat "$output_file"
              else
                echo "Output file not found: $output_file"
              fi
              echo
            done

            if [ $success != "true" ]; then
              echo "ERROR: At least one S3 upload in the last batch failed"
              exit 1
            fi

            # Reset job arrays for the next batch
            jobs=()
            job_outputs=()
          fi
        done

    - name: push-regeneration-data-to-s3
      image: quay.io/konflux-ci/release-service-utils:e85ceb962ee6f4d0672b4aa4e9946621ab302f20
      computeResources:
        limits:
          memory: 32Mi
        requests:
          memory: 32Mi
          cpu: 10m
      volumeMounts:
        - name: aws-secret
          mountPath: /secrets
      script: |
        #!/usr/bin/env bash
        set -o pipefail -o nounset

        if [ -z "$(params.dataPath)" ]; then
          echo "Release data path not provided, skipping regeneration data push."
          exit 0
        fi

        if [ -z "$(params.snapshotSpec)" ]; then
          echo "Snapshot spec path not provided, skipping regeneration data push."
          exit 0
        fi

        if [ -z "$(params.releaseId)" ]; then
          echo "Snapshot spec path not provided, skipping regeneration data push."
          exit 0
        fi

        aws_access_key_id="$(cat /secrets/atlas-aws-access-key-id)"
        aws_secret_access_key="$(cat /secrets/atlas-aws-secret-access-key)"

        region="us-east-1"
        bucket="$(params.retryS3Bucket)"

        push_s3() {
          prefix="$1"
          file="$2"

          s3_filename="${prefix}/$(params.releaseId)"
          resource="/${bucket}/${s3_filename}"

          echo "Pushing $file to $prefix"
          content_type="application/json"
          date_value=$(date -R)
          to_sign="PUT\n\n${content_type}\n${date_value}\n${resource}"
          signature=$(echo -en "${to_sign}" | openssl sha1 -hmac "${aws_secret_access_key}" -binary | base64)

          if ! curl -X PUT --upload-file "$file" \
              --silent --show-error --fail-with-body --retry 10 \
              --retry-all-errors \
              -H "Host: ${bucket}.s3.${region}.amazonaws.com" \
              -H "Date: ${date_value}" \
              -H "Content-Type: ${content_type}" \
              -H "Authorization: AWS ${aws_access_key_id}:${signature}" \
              "https://${bucket}.s3.${region}.amazonaws.com/${s3_filename}"; then
            >&2 echo "ERROR: Failed to push SBOM regeneration data to S3 bucket."
            exit 1
          fi
        }

        data_path="$(params.dataDir)/$(params.dataPath)"
        snapshot_path="$(params.dataDir)/$(params.snapshotSpec)"

        push_s3 "release-data" "$data_path"
        push_s3 "snapshots" "$snapshot_path"

    - name: create-trusted-artifact
      computeResources:
        limits:
          memory: 128Mi
        requests:
          memory: 128Mi
          cpu: 250m
      ref:
        resolver: "git"
        params:
          - name: url
            value: "$(params.taskGitUrl)"
          - name: revision
            value: "$(params.taskGitRevision)"
          - name: pathInRepo
            value: stepactions/create-trusted-artifact/create-trusted-artifact.yaml
      params:
        - name: ociStorage
          value: $(params.ociStorage)
        - name: workDir
          value: $(params.dataDir)
        - name: sourceDataArtifact
          value: $(results.sourceDataArtifact.path)
    - name: patch-source-data-artifact-result
      computeResources:
        limits:
          memory: 32Mi
        requests:
          memory: 32Mi
          cpu: 20m
      ref:
        resolver: "git"
        params:
          - name: url
            value: $(params.taskGitUrl)
          - name: revision
            value: $(params.taskGitRevision)
          - name: pathInRepo
            value: stepactions/patch-source-data-artifact-result/patch-source-data-artifact-result.yaml
      params:
        - name: ociStorage
          value: $(params.ociStorage)
        - name: sourceDataArtifact
          value: $(results.sourceDataArtifact.path)
